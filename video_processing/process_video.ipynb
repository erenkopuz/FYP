{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52d3d61c31dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprep_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_to_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletterbox_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image, letterbox_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim\n",
    "\n",
    "def write(x, img):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = random.choice(colors)\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"video.avi\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"cfg/yolov3.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"yolov3.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
    "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
    "                        default = \"416\", type = str)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--video VIDEO] [--dataset DATASET]\n",
      "                             [--confidence CONFIDENCE]\n",
      "                             [--nms_thresh NMS_THRESH] [--cfg CFGFILE]\n",
      "                             [--weights WEIGHTSFILE] [--reso RESO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/Eren/Library/Jupyter/runtime/kernel-16db8a81-c358-43b1-a8db-14ac9259388c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "confidence = float(0.7)\n",
    "nms_thesh = float(0.4)\n",
    "start = 0\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "bbox_attrs = 5 + num_classes\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(args.cfgfile)\n",
    "model.load_weights(args.weightsfile)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = args.reso\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model(get_test_input(inp_dim, CUDA), CUDA)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "videofile = args.video\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frames = 0\n",
    "start = time.time()    \n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "\n",
    "        img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "\n",
    "        im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
    "\n",
    "\n",
    "        if CUDA:\n",
    "            im_dim = im_dim.cuda()\n",
    "            img = img.cuda()\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            output = model(Variable(img), CUDA)\n",
    "        output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "\n",
    "        if type(output) == int:\n",
    "            frames += 1\n",
    "            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "            cv2.imshow(\"frame\", orig_im)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        im_dim = im_dim.repeat(output.size(0), 1)\n",
    "        scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
    "\n",
    "        output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "        output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "\n",
    "        output[:,1:5] /= scaling_factor\n",
    "\n",
    "        for i in range(output.shape[0]):\n",
    "            output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "            output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "\n",
    "        classes = load_classes('data/coco.names')\n",
    "        colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "        list(map(lambda x: write(x, orig_im), output))\n",
    "\n",
    "\n",
    "        cv2.imshow(\"frame\", orig_im)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "        frames += 1\n",
    "        print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some code sourced from:\n",
    "\n",
    "#############################################\n",
    "# Object detection - YOLO - OpenCV\n",
    "# Author : Arun Ponnusamy   (July 16, 2018)\n",
    "# Website : http://www.arunponnusamy.com\n",
    "############################################\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def get_yolo_output(image):\n",
    "\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    scale = 0.00392\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    boxes_nms = []\n",
    "    class_ids_nms = []\n",
    "    confidences_nms = []\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        boxes_nms.append(boxes[i])\n",
    "        class_ids_nms.append(class_ids[i])\n",
    "        confidences_nms.append(confidences[i])\n",
    "    \n",
    "    return boxes_nms, class_ids_nms, confidences_nms\n",
    "\n",
    "\n",
    "def print_on_image(image, boxes_nms, class_ids_nms, confidences_nms):\n",
    "\n",
    "    cv2.polylines(image,swalk_pts,True,(0,255,255),thickness=3)\n",
    "    cv2.polylines(image,lane_pts,True,(255,0,255),thickness=3)\n",
    "    \n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_prediction(image, class_ids_nms[i], confidences_nms[i], round(x), round(y), round(x+w), round(y+h))\n",
    "        \n",
    "\n",
    "        \n",
    "def create_grid_image(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor, grid_background):\n",
    "    \n",
    "    \n",
    "    grid_vehicles   = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_person     = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_bicycle    = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "\n",
    "        x_plus_w = round(x+w)\n",
    "        y_plus_h = round(y+h)\n",
    "\n",
    "\n",
    "        x = round(x/grid_scaling_factor)\n",
    "        y = round(y/grid_scaling_factor)\n",
    "        x_plus_w = round(x_plus_w/grid_scaling_factor)\n",
    "        y_plus_h = round(y_plus_h/grid_scaling_factor)\n",
    "        \n",
    "        if class_ids_nms[i] in [2, 3, 5, 6, 7]:  # Motorized vehicles [car, motorcycle, bus, train, truck]\n",
    "            \n",
    "            cv2.rectangle(grid_vehicles, (x,y), (x_plus_w,y_plus_h), vehicle_w, -1)\n",
    "\n",
    "\n",
    "        elif class_ids_nms[i]==0:  # Person\n",
    "            \n",
    "            cv2.rectangle(grid_person, (x,y), (x_plus_w,y_plus_h), person_w, -1) \n",
    "            \n",
    "        elif class_ids_nms[i]==1:  # Bicycle\n",
    "            \n",
    "            cv2.rectangle(grid_bicycle, (x,y), (x_plus_w,y_plus_h), bicycle_w, -1)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    grid_overflow = np.zeros((grid_y_size,grid_x_size), dtype = np.int16)    \n",
    "    grid_overflow = np.array([grid_background,grid_vehicles,grid_person,grid_bicycle])\n",
    "    grid_overflow = grid_overflow.sum(axis=0)\n",
    "    grid_overflow[grid_overflow>127] = 127\n",
    "    grid_overflow[grid_overflow<-128] = -128\n",
    "    grid = np.array(grid_overflow,dtype=np.int8)\n",
    "\n",
    "    return grid\n",
    "       \n",
    "\n",
    "    \n",
    "def create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor, grid_background,\n",
    "               vehicles=False,\n",
    "               persons=False,\n",
    "               bicycles=False):\n",
    "    \n",
    "    options = [vehicles, persons, bicycles]\n",
    "    num_channels = options.count(True)\n",
    "    \n",
    "    grid = np.zeros((num_channels,grid_y_size,grid_x_size), dtype = bool)\n",
    "    \n",
    "    \n",
    "    grid_vehicles   = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    grid_person     = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    grid_bicycle    = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    \n",
    "    present_val = 1\n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "\n",
    "        x_plus_w = round(x+w)\n",
    "        y_plus_h = round(y+h)\n",
    "\n",
    "\n",
    "        x = round(x/grid_scaling_factor)\n",
    "        y = round(y/grid_scaling_factor)\n",
    "        x_plus_w = round(x_plus_w/grid_scaling_factor)\n",
    "        y_plus_h = round(y_plus_h/grid_scaling_factor)\n",
    "        \n",
    "        \n",
    "        if class_ids_nms[i] in [2, 3, 5, 6, 7]:  # Motorized vehicles [car, motorcycle, bus, train, truck]\n",
    "            \n",
    "            grid_vehicles[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "\n",
    "        elif class_ids_nms[i]==0:  # Person\n",
    "            \n",
    "            grid_person[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "            \n",
    "        elif class_ids_nms[i]==1:  # Bicycle\n",
    "            \n",
    "            grid_bicycle[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    j = 0\n",
    "    if vehicles:\n",
    "        grid[j] = grid_vehicles\n",
    "        j = j+1\n",
    "    if persons:\n",
    "        grid[j] = grid_person\n",
    "        j = j+1\n",
    "    if bicycles:\n",
    "        grid[j] = grid_bicycle\n",
    "        j = j+1\n",
    "    \n",
    "    return grid\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial and Time Scaling Coefficients\n",
    "* \"seconds\" represents the time intervals in seconds at which samples will be taken from the video.    \n",
    "* \"grid_scaling_factor\" represents the factor by which the grid will be shrunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = 5\n",
    "grid_scaling_factor = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Background Areas #############\n",
    "\n",
    "#lanes\n",
    "\n",
    "lane_pts = []\n",
    "lane_pts_1 = np.array([[540,0],[483,303],[314,555],[0,915],[0,1080],[625,1080],[708,719],[745,544],[763,362],[687,154],[791,310],[822,396],[835,582],[806,861],[779,1080],[1114,1080],[1060,664],[988,418],[924,317],[662,0]], np.int32)\n",
    "lane_pts_1 = lane_pts_1.reshape((-1,1,2))\n",
    "\n",
    "lane_pts_2 = np.array([[1920,600],[1832,660],[1760,773],[1692,907],[1660,1080],[1140,1080],[1245,893],[1418,691],[1590,569],[1782,480],[1920,421]], np.int32)\n",
    "lane_pts_2 = lane_pts_2.reshape((-1,1,2))\n",
    "\n",
    "lane_pts.append(lane_pts_1)\n",
    "lane_pts.append(lane_pts_2)\n",
    "\n",
    "\n",
    "lane_pts_grid = []\n",
    "\n",
    "for pts in lane_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    lane_pts_grid.append(grid_pts)\n",
    "\n",
    "\n",
    "\n",
    "#sidewalks\n",
    "\n",
    "swalk_pts = []\n",
    "swalk_pts_1 = np.array([[454,0],[540,0],[514,143],[448,149],[383,306],[483,303],[314,555],[0,915],[0,425],[179,356]], np.int32)\n",
    "swalk_pts_1 = swalk_pts_1.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_2 = np.array([[986,265],[1173,264],[1268,319],[1484,291],[1600,296],[1920,224],[1920,281],[1280,529],[1151,500]], np.int32)\n",
    "swalk_pts_2 = swalk_pts_2.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_3 = np.array([[1920,600],[1920,1080],[1660,1080],[1692,907],[1760,773],[1832,660]], np.int32)\n",
    "swalk_pts_3 = swalk_pts_3.reshape((-1,1,2))\n",
    "\n",
    "\n",
    "swalk_pts.append(swalk_pts_1)\n",
    "swalk_pts.append(swalk_pts_2)\n",
    "swalk_pts.append(swalk_pts_3)\n",
    "\n",
    "\n",
    "swalk_pts_grid = []\n",
    "\n",
    "for pts in swalk_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    swalk_pts_grid.append(grid_pts)\n",
    "    \n",
    "\n",
    "\n",
    "#out of bounds (oob) areas\n",
    "\n",
    "\n",
    "#median spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Grid Weights #############\n",
    "\n",
    "\n",
    "lane_w = 32\n",
    "swalk_w = -32\n",
    "oob_w = -127\n",
    "med_sp_w = 0\n",
    "\n",
    "\n",
    "vehicle_w = 64\n",
    "person_w = -64\n",
    "bicycle_w = 32\n",
    "\n",
    "# obj_w = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################################### \n",
    "\n",
    "classes = None\n",
    "\n",
    "with open('yolov3.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "GREYSCALE = np.random.uniform(0, 255, size=(len(classes)))\n",
    "\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "\n",
    "#image = cv2.imread('dog.jpg')\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS:  23.976023976023978\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "videofile = 'videos/DSC_0005.MOV'\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "success,image = cap.read()\n",
    "\n",
    "\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) # Gets the total number of frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # Gets the frame width\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Gets the frame width\n",
    "\n",
    "grid_x_size = int(round(frame_width/grid_scaling_factor))\n",
    "grid_y_size = int(round(frame_height/grid_scaling_factor))\n",
    "\n",
    "\n",
    "multiplier = int(round(fps * seconds))\n",
    "num_scenes = int(round(num_frames/(fps * seconds))+1)\n",
    "\n",
    "\n",
    "grids_v       = np.zeros((num_scenes,1,grid_y_size,grid_x_size), dtype = bool)\n",
    "grids_p       = np.zeros((num_scenes,1,grid_y_size,grid_x_size), dtype = bool)\n",
    "grids_vp      = np.zeros((num_scenes,2,grid_y_size,grid_x_size), dtype = bool)\n",
    "grids_vpb     = np.zeros((num_scenes,3,grid_y_size,grid_x_size), dtype = bool)\n",
    "\n",
    "\n",
    "frameIds    = np.zeros((num_scenes), dtype = np.int32)\n",
    "time_sec_count = np.zeros((num_scenes), dtype = np.int32)\n",
    "\n",
    "\n",
    "#################### Initiate Process ################\n",
    "\n",
    "#################### Create Background Grid ################\n",
    "\n",
    "grid_background = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "\n",
    "# Mark background\n",
    "cv2.fillPoly(grid_background,lane_pts_grid,lane_w)\n",
    "cv2.fillPoly(grid_background,swalk_pts_grid,swalk_w)\n",
    "\n",
    "print('FPS: ', fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(2, 34, 60)\n",
      "5\n",
      "(2, 34, 60)\n",
      "10\n",
      "(2, 34, 60)\n",
      "15\n",
      "(2, 34, 60)\n",
      "20\n",
      "(2, 34, 60)\n",
      "25\n",
      "(2, 34, 60)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-f319e35cdc53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mboxes_nms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids_nms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidences_nms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yolo_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msecond_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeId\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-dc8f301e81e2>\u001b[0m in \u001b[0;36mget_yolo_output\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_output_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "i = 0\n",
    "\n",
    "while success:\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        \n",
    "\n",
    "        \n",
    "#############################################################\n",
    "\n",
    "        \n",
    "        boxes_nms, class_ids_nms, confidences_nms = get_yolo_output(image)\n",
    "        \n",
    "        second_count = round(frameId/fps)\n",
    "        print (second_count)\n",
    "        \n",
    "        grid = create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor,grid_background,\n",
    "                           vehicles = True,\n",
    "                           persons = True,\n",
    "                           bicycles = True)\n",
    "        \n",
    "        \n",
    "        print (grid[:2].shape)\n",
    "        \n",
    "        grids_v[i] = grid[0]\n",
    "        grids_p[i] = grid[1]\n",
    "        grids_vp[i] = grid[:2]\n",
    "        grids_vpb[i] = grid\n",
    "        \n",
    "        \n",
    "        \n",
    "        frameIds[i] = frameId\n",
    "        time_sec_count[i] = second_count\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if frameId % (multiplier) == 0:\n",
    "            \n",
    "            \n",
    "            grid_rep = np.array(grid[1],dtype=np.uint8)\n",
    "            grid_rep = np.multiply(grid_rep,255)\n",
    "            cv2.imwrite(\"output/representations/rep%d.jpg\" % frameId, grid_rep)\n",
    "        \n",
    "            grid_image = create_grid_image(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor,grid_background)\n",
    "            grid_image_16_bit = np.array(grid_image,dtype=np.int16)\n",
    "            grid_image_16_bit = np.add(grid_image_16_bit,128)\n",
    "            grid_image_unsigned = np.array(grid_image_16_bit,dtype=np.uint8)\n",
    "            cv2.imwrite(\"output/grids/grid%d.jpg\" % frameId, grid_image_unsigned)\n",
    "\n",
    "            print_on_image(image, boxes_nms, class_ids_nms, confidences_nms)\n",
    "\n",
    "            cv2.imwrite(\"output/frames/frame%d.jpg\" % frameId, image)\n",
    "        \n",
    "        \n",
    "        i= i+1\n",
    "        \n",
    "        #print ('One image done')\n",
    "        \n",
    "        \n",
    "    frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "    success, image = cap.read()\n",
    "\n",
    "\n",
    "        #plt.figure(figsize = (15, 15))\n",
    "        #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    \n",
    "#cv2.imwrite(\"object-detection.jpg\", image)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "print (i)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "\n",
    "\n",
    "print (\"Complete\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"grid_arrays/grids_v.npy\", grids_v)\n",
    "np.save(\"grid_arrays/grids_p.npy\", grids_p)\n",
    "np.save(\"grid_arrays/grids_vp.npy\", grids_vp)\n",
    "np.save(\"grid_arrays/grids_vpb.npy\", grids_vpb)\n",
    "\n",
    "np.save(\"grid_arrays/frameIds.npy\", frameIds)\n",
    "np.save(\"grid_arrays/time_sec_count.npy\", time_sec_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = np.load(\"grid_arrays/grids.npy\")\n",
    "frameIds = np.load(\"grid_arrays/frameIds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "j = 0\n",
    "for i in grids[2].flat:\n",
    "    if i == True:\n",
    "        j = j+1\n",
    "        \n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithms on Grids\n",
    "## Divergence from Average Inquiry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080\n",
      "3708\n",
      "3732\n",
      "3744\n",
      "3792\n",
      "3804\n",
      "3816\n",
      "3828\n",
      "3840\n",
      "3852\n",
      "3864\n",
      "3876\n",
      "3888\n",
      "3900\n",
      "3912\n",
      "3924\n",
      "3936\n",
      "7452\n",
      "7464\n",
      "7500\n",
      "7512\n",
      "7524\n",
      "7536\n",
      "7548\n",
      "7560\n",
      "7572\n",
      "7584\n",
      "7656\n",
      "7812\n",
      "7848\n",
      "7860\n",
      "7872\n",
      "7884\n",
      "7896\n",
      "7908\n",
      "7920\n",
      "7932\n",
      "11172\n",
      "11184\n",
      "12120\n",
      "12480\n",
      "12540\n",
      "12624\n",
      "13068\n",
      "13080\n",
      "13200\n",
      "13212\n",
      "13248\n",
      "13272\n",
      "13728\n",
      "13740\n",
      "13752\n",
      "13764\n",
      "128.67264789749873\n"
     ]
    }
   ],
   "source": [
    "mean_grid = np.mean(grids, axis=0)\n",
    "\n",
    "threshold = 200\n",
    "\n",
    "dists = 0\n",
    "j = 0\n",
    "for grid,frameId  in zip(grids,frameIds):\n",
    "    grid_numeric = np.array(grid, dtype = np.float64)\n",
    "    var = distance.sqeuclidean(np.ravel(grid_numeric),np.ravel(mean_grid))\n",
    "    dists = dists + var\n",
    "    if var > threshold:\n",
    "        j = j+1\n",
    "        print(frameId)\n",
    "\n",
    "\n",
    "mean = dists/frameIds.size\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_grid = np.multiply(mean_grid,255)\n",
    "cv2.imwrite(\"mean_grid.jpg\", mean_grid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178.98834304746043\n"
     ]
    }
   ],
   "source": [
    "maxi = np.amax(mean_grid)\n",
    "print (maxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.2\n",
    "\n",
    "train_end_ind = round((1-test_size)*len(grids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1201, 34, 60)\n",
      "1201\n",
      "(1201, 34, 60)\n",
      "(961, 2040)\n",
      "(240, 2040)\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "print(grids.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = grids.reshape((len(grids), np.prod(grids.shape[1:])))[:train_end_ind], grids.reshape((len(grids), np.prod(grids.shape[1:])))[train_end_ind:]\n",
    "Id_train, Id_test = frameIds[:train_end_ind], frameIds[train_end_ind:]\n",
    "\n",
    "print(len(grids))\n",
    "\n",
    "#X_train, X_test, Id_train, Id_test = train_test_split(grids.reshape((len(grids), np.prod(grids.shape[1:]))), frameIds, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "print(grids.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(34*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n"
     ]
    }
   ],
   "source": [
    "input_dim = np.prod(grids.shape[1:])\n",
    "encoding_dim = 14\n",
    "\n",
    "print(input_dim)\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 961 samples, validate on 240 samples\n",
      "Epoch 1/100\n",
      "961/961 [==============================] - 2s 2ms/step - loss: 0.0871 - acc: 0.0010 - val_loss: 0.1048 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0746 - acc: 0.0042 - val_loss: 0.0901 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "961/961 [==============================] - 0s 259us/step - loss: 0.0679 - acc: 0.0010 - val_loss: 0.0864 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "961/961 [==============================] - 0s 257us/step - loss: 0.0648 - acc: 0.0000e+00 - val_loss: 0.0850 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0631 - acc: 0.0000e+00 - val_loss: 0.0830 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0616 - acc: 0.0000e+00 - val_loss: 0.0819 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0605 - acc: 0.0000e+00 - val_loss: 0.0829 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0600 - acc: 0.0000e+00 - val_loss: 0.0801 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "961/961 [==============================] - 0s 240us/step - loss: 0.0596 - acc: 0.0000e+00 - val_loss: 0.0808 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0586 - acc: 0.0000e+00 - val_loss: 0.0795 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0580 - acc: 0.0000e+00 - val_loss: 0.0794 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0591 - acc: 0.0000e+00 - val_loss: 0.0824 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0585 - acc: 0.0000e+00 - val_loss: 0.0811 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0578 - acc: 0.0000e+00 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0574 - acc: 0.0000e+00 - val_loss: 0.0794 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0571 - acc: 0.0000e+00 - val_loss: 0.0793 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0570 - acc: 0.0000e+00 - val_loss: 0.0792 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0568 - acc: 0.0000e+00 - val_loss: 0.0791 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "961/961 [==============================] - 0s 256us/step - loss: 0.0561 - acc: 0.0000e+00 - val_loss: 0.0811 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0563 - acc: 0.0000e+00 - val_loss: 0.0801 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "961/961 [==============================] - 0s 250us/step - loss: 0.0556 - acc: 0.0000e+00 - val_loss: 0.0783 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "961/961 [==============================] - 0s 253us/step - loss: 0.0550 - acc: 0.0000e+00 - val_loss: 0.0786 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0547 - acc: 0.0010 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0543 - acc: 0.0010 - val_loss: 0.0779 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0550 - acc: 0.0021 - val_loss: 0.0807 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0548 - acc: 0.0042 - val_loss: 0.0798 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0530 - acc: 0.0042 - val_loss: 0.0777 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0549 - acc: 0.0021 - val_loss: 0.0808 - val_acc: 0.0083\n",
      "Epoch 29/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0532 - acc: 0.0042 - val_loss: 0.0796 - val_acc: 0.0042\n",
      "Epoch 30/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0527 - acc: 0.0073 - val_loss: 0.0787 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0523 - acc: 0.0094 - val_loss: 0.0815 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0517 - acc: 0.0073 - val_loss: 0.0791 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0513 - acc: 0.0073 - val_loss: 0.0849 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0541 - acc: 0.0062 - val_loss: 0.0794 - val_acc: 0.0042\n",
      "Epoch 35/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0521 - acc: 0.0094 - val_loss: 0.0789 - val_acc: 0.0042\n",
      "Epoch 36/100\n",
      "961/961 [==============================] - 0s 241us/step - loss: 0.0524 - acc: 0.0114 - val_loss: 0.0785 - val_acc: 0.0083\n",
      "Epoch 37/100\n",
      "961/961 [==============================] - 0s 240us/step - loss: 0.0512 - acc: 0.0125 - val_loss: 0.0800 - val_acc: 0.0042\n",
      "Epoch 38/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0509 - acc: 0.0073 - val_loss: 0.0781 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0509 - acc: 0.0135 - val_loss: 0.0785 - val_acc: 0.0083\n",
      "Epoch 40/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0509 - acc: 0.0094 - val_loss: 0.0783 - val_acc: 0.0042\n",
      "Epoch 41/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0505 - acc: 0.0125 - val_loss: 0.0767 - val_acc: 0.0042\n",
      "Epoch 42/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0502 - acc: 0.0125 - val_loss: 0.0787 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0499 - acc: 0.0073 - val_loss: 0.0822 - val_acc: 0.0083\n",
      "Epoch 44/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0505 - acc: 0.0114 - val_loss: 0.0786 - val_acc: 0.0042\n",
      "Epoch 45/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0497 - acc: 0.0125 - val_loss: 0.0780 - val_acc: 0.0042\n",
      "Epoch 46/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0495 - acc: 0.0166 - val_loss: 0.0788 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0499 - acc: 0.0146 - val_loss: 0.0804 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "961/961 [==============================] - 0s 238us/step - loss: 0.0515 - acc: 0.0125 - val_loss: 0.0812 - val_acc: 0.0125\n",
      "Epoch 49/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0503 - acc: 0.0135 - val_loss: 0.0778 - val_acc: 0.0083\n",
      "Epoch 50/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0513 - acc: 0.0125 - val_loss: 0.0783 - val_acc: 0.0125\n",
      "Epoch 51/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0497 - acc: 0.0146 - val_loss: 0.0763 - val_acc: 0.0042\n",
      "Epoch 52/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0488 - acc: 0.0156 - val_loss: 0.0767 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0501 - acc: 0.0062 - val_loss: 0.0788 - val_acc: 0.0125\n",
      "Epoch 54/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0514 - acc: 0.0083 - val_loss: 0.0801 - val_acc: 0.0042\n",
      "Epoch 55/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0525 - acc: 0.0062 - val_loss: 0.0796 - val_acc: 0.0042\n",
      "Epoch 56/100\n",
      "961/961 [==============================] - 0s 260us/step - loss: 0.0498 - acc: 0.0073 - val_loss: 0.0796 - val_acc: 0.0083\n",
      "Epoch 57/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0500 - acc: 0.0062 - val_loss: 0.0764 - val_acc: 0.0042\n",
      "Epoch 58/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0487 - acc: 0.0094 - val_loss: 0.0778 - val_acc: 0.0125\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961/961 [==============================] - 0s 240us/step - loss: 0.0485 - acc: 0.0114 - val_loss: 0.0790 - val_acc: 0.0125\n",
      "Epoch 60/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0491 - acc: 0.0083 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0514 - acc: 0.0031 - val_loss: 0.0775 - val_acc: 0.0042\n",
      "Epoch 62/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0490 - acc: 0.0094 - val_loss: 0.0770 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0489 - acc: 0.0062 - val_loss: 0.0777 - val_acc: 0.0042\n",
      "Epoch 64/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0480 - acc: 0.0052 - val_loss: 0.0769 - val_acc: 0.0042\n",
      "Epoch 65/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0475 - acc: 0.0062 - val_loss: 0.0789 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0489 - acc: 0.0021 - val_loss: 0.0779 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0480 - acc: 0.0104 - val_loss: 0.0770 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0484 - acc: 0.0031 - val_loss: 0.0772 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0475 - acc: 0.0021 - val_loss: 0.0774 - val_acc: 0.0042\n",
      "Epoch 70/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0472 - acc: 0.0021 - val_loss: 0.0757 - val_acc: 0.0042\n",
      "Epoch 71/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0467 - acc: 0.0042 - val_loss: 0.0758 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0464 - acc: 0.0031 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "961/961 [==============================] - 0s 247us/step - loss: 0.0464 - acc: 0.0010 - val_loss: 0.0771 - val_acc: 0.0083\n",
      "Epoch 74/100\n",
      "961/961 [==============================] - 0s 241us/step - loss: 0.0467 - acc: 0.0062 - val_loss: 0.0754 - val_acc: 0.0042\n",
      "Epoch 75/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0469 - acc: 0.0042 - val_loss: 0.0770 - val_acc: 0.0042\n",
      "Epoch 76/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0464 - acc: 0.0031 - val_loss: 0.0754 - val_acc: 0.0042\n",
      "Epoch 77/100\n",
      "961/961 [==============================] - 0s 252us/step - loss: 0.0460 - acc: 0.0062 - val_loss: 0.0752 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0463 - acc: 0.0062 - val_loss: 0.0771 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0466 - acc: 0.0052 - val_loss: 0.0760 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0463 - acc: 0.0052 - val_loss: 0.0767 - val_acc: 0.0042\n",
      "Epoch 81/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0461 - acc: 0.0062 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0489 - acc: 0.0000e+00 - val_loss: 0.0778 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0480 - acc: 0.0021 - val_loss: 0.0758 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0468 - acc: 0.0042 - val_loss: 0.0761 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "961/961 [==============================] - 0s 248us/step - loss: 0.0470 - acc: 0.0010 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0467 - acc: 0.0021 - val_loss: 0.0763 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "961/961 [==============================] - 0s 242us/step - loss: 0.0459 - acc: 0.0062 - val_loss: 0.0761 - val_acc: 0.0042\n",
      "Epoch 88/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0466 - acc: 0.0052 - val_loss: 0.0768 - val_acc: 0.0042\n",
      "Epoch 89/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0469 - acc: 0.0021 - val_loss: 0.0760 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0468 - acc: 0.0042 - val_loss: 0.0762 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "961/961 [==============================] - 0s 238us/step - loss: 0.0459 - acc: 0.0010 - val_loss: 0.0771 - val_acc: 0.0042\n",
      "Epoch 92/100\n",
      "961/961 [==============================] - 0s 239us/step - loss: 0.0455 - acc: 0.0021 - val_loss: 0.0760 - val_acc: 0.0042\n",
      "Epoch 93/100\n",
      "961/961 [==============================] - 0s 246us/step - loss: 0.0452 - acc: 0.0052 - val_loss: 0.0763 - val_acc: 0.0042\n",
      "Epoch 94/100\n",
      "961/961 [==============================] - 0s 244us/step - loss: 0.0454 - acc: 0.0000e+00 - val_loss: 0.0764 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "961/961 [==============================] - 0s 245us/step - loss: 0.0456 - acc: 0.0000e+00 - val_loss: 0.0770 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "961/961 [==============================] - 0s 249us/step - loss: 0.0459 - acc: 0.0021 - val_loss: 0.0763 - val_acc: 0.0042\n",
      "Epoch 97/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0454 - acc: 0.0010 - val_loss: 0.0762 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "961/961 [==============================] - 0s 241us/step - loss: 0.0460 - acc: 0.0010 - val_loss: 0.0770 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "961/961 [==============================] - 0s 243us/step - loss: 0.0455 - acc: 0.0021 - val_loss: 0.0772 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "961/961 [==============================] - 0s 240us/step - loss: 0.0458 - acc: 0.0021 - val_loss: 0.0787 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"models/model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4VGX2wPHvSSeFdHqA0KWX0ARFUaooa8OGZXXF/e26trWgu+rqrq5l7R0VO4JiQwFBihQpEjrSEnoAIQkhhJCe9/fHO4FJmFQymZCcz/PkYea2OTcT7rlvvWKMQSmllCqLl6cDUEopVftpslBKKVUuTRZKKaXKpclCKaVUuTRZKKWUKpcmC6WUUuXSZKHUGRKRD0XkPxXcdreIXHymx1GqpmmyUEopVS5NFkoppcqlyULVC47qnwdEZIOIZIrI+yLSWERmi0iGiMwTkXCn7S8Tkd9E5KiI/Cwi5zit6yUiaxz7TQMCSnzWGBFZ59h3mYh0r2LMt4tIoogcEZEZItLMsVxE5CUROSwi6Y5z6upYN1pENjti2y8i91fpF6ZUCZosVH1yJTAM6ABcCswGHgGisP8X7gIQkQ7A58A9QDQwC/heRPxExA/4FvgEiAC+dBwXx769gcnAHUAk8A4wQ0T8KxOoiAwF/guMA5oCe4CpjtXDgfMd5xEGXAOkOta9D9xhjAkBugILKvO5SpVGk4WqT14zxhwyxuwHlgArjTFrjTE5wDdAL8d21wAzjTE/GWPygP8BDYBzgQGAL/CyMSbPGDMdWOX0GbcD7xhjVhpjCowxHwE5jv0q4wZgsjFmjSO+h4GBItIayANCgE6AGGO2GGMOOvbLAzqLSENjTJoxZk0lP1cplzRZqPrkkNPrLBfvgx2vm2Hv5AEwxhQC+4DmjnX7TfEZOPc4vW4F/N1RBXVURI4CMY79KqNkDMexpYfmxpgFwOvAG8AhEZkkIg0dm14JjAb2iMgiERlYyc9VyiVNFkqd7gD2og/YNgLsBX8/cBBo7lhWpKXT633AU8aYMKefQGPM52cYQxC2Wms/gDHmVWNMH6ALtjrqAcfyVcaYsUAjbHXZF5X8XKVc0mSh1Om+AC4RkYtExBf4O7YqaRmwHMgH7hIRHxG5AujntO+7wJ9FpL+jITpIRC4RkZBKxjAF+KOI9HS0dzyNrTbbLSJ9Hcf3BTKBbKDA0aZyg4iEOqrPjgEFZ/B7UOokTRZKlWCM2QaMB14DUrCN4ZcaY3KNMbnAFcAtQBq2feNrp33jse0WrzvWJzq2rWwM84FHga+wpZm2wLWO1Q2xSSkNW1WVim1XAbgR2C0ix4A/O85DqTMm+vAjpZRS5dGShVJKqXJpslBKKVUuTRZKKaXKpclCKaVUuXw8HUB1iYqKMq1bt/Z0GEopdVZZvXp1ijEmurzt6kyyaN26NfHx8Z4OQymlzioisqf8rbQaSimlVAVoslBKKVUuTRZKKaXKVWfaLJRSqiry8vJISkoiOzvb06G4VUBAAC1atMDX17dK+2uyUErVa0lJSYSEhNC6dWuKTyZcdxhjSE1NJSkpidjY2CodQ6uhlFL1WnZ2NpGRkXU2UQCICJGRkWdUetJkoZSq9+pyoihypueoySI9CRY8Bak7PB2JUkrVWposMlNg8XNweIunI1FK1UNHjx7lzTffrPR+o0eP5ujRo26IyDVNFgGh9t/sdM/GoZSql0pLFgUFZT/kcNasWYSFhbkrrNNobyhNFkopD5o4cSI7duygZ8+e+Pr6EhwcTNOmTVm3bh2bN2/mD3/4A/v27SM7O5u7776bCRMmAKemODp+/DijRo1i8ODBLFu2jObNm/Pdd9/RoEGDao1Tk4UmC6WUwxPf/8bmA8eq9ZidmzXk8Uu7lLr+mWeeYdOmTaxbt46ff/6ZSy65hE2bNp3s4jp58mQiIiLIysqib9++XHnllURGRhY7RkJCAp9//jnvvvsu48aN46uvvmL8+Op9oq4mCy9v8G+oyUIpVSv069ev2FiIV199lW+++QaAffv2kZCQcFqyiI2NpWfPngD06dOH3bt3V3tcmizAli40WShV75VVAqgpQUFBJ1///PPPzJs3j+XLlxMYGMgFF1zgcqyEv7//ydfe3t5kZWVVe1zawA2OZFFzvQqUUqpISEgIGRkZLtelp6cTHh5OYGAgW7duZcWKFTUc3SlasgAtWSilPCYyMpJBgwbRtWtXGjRoQOPGjU+uGzlyJG+//Tbdu3enY8eODBgwwGNxarIAmyyO7vV0FEqpemrKlCkul/v7+zN79myX64raJaKioti0adPJ5ffff3+1xwdaDWVpyUIppcqkyQIgIEyThVJKlUGTBdiSRc4xKCx7xKRSStVXmixAB+YppVQ53JosRGSkiGwTkUQRmehi/fkiskZE8kXkqhLrbhaRBMfPze6MU5OFUkqVzW3JQkS8gTeAUUBn4DoR6Vxis73ALcCUEvtGAI8D/YF+wOMiEu6uWDVZKKVU2dxZsugHJBpjdhpjcoGpwFjnDYwxu40xG4DCEvuOAH4yxhwxxqQBPwEj3RapJgullIdUdYpygJdffpkTJ05Uc0SuuTNZNAf2Ob1Pciyrtn1FZIKIxItIfHJycpUDpYFjml9NFkqpGna2JAt3Dspz9Qw/U537GmMmAZMA4uLiKnrs02nJQinlIc5TlA8bNoxGjRrxxRdfkJOTw+WXX84TTzxBZmYm48aNIykpiYKCAh599FEOHTrEgQMHuPDCC4mKimLhwoVujdOdySIJiHF63wI4UIl9Lyix78/VEpUrJ5OFzg+lVL02eyL8vrF6j9mkG4x6ptTVzlOUz507l+nTp/Prr79ijOGyyy5j8eLFJCcn06xZM2bOnAnYOaNCQ0N58cUXWbhwIVFRUdUbswvurIZaBbQXkVgR8QOuBWZUcN85wHARCXc0bA93LHMPvxBAtGShlPKouXPnMnfuXHr16kXv3r3ZunUrCQkJdOvWjXnz5vHQQw+xZMkSQkNDazw2t5UsjDH5InIn9iLvDUw2xvwmIk8C8caYGSLSF/gGCAcuFZEnjDFdjDFHROTf2IQD8KQx5oi7YsXLCwL0mRZK1XtllABqgjGGhx9+mDvuuOO0datXr2bWrFk8/PDDDB8+nMcee6xGY3PrRILGmFnArBLLHnN6vQpbxeRq38nAZHfGV4zOD6WU8gDnKcpHjBjBo48+yg033EBwcDD79+/H19eX/Px8IiIiGD9+PMHBwXz44YfF9q2JaiiddbaIzg+llPIA5ynKR40axfXXX8/AgQMBCA4O5tNPPyUxMZEHHngALy8vfH19eeuttwCYMGECo0aNomnTpm5v4BZjqt6JqDaJi4sz8fHxVT/Ah2OgMB9u/bH6glJK1XpbtmzhnHPO8XQYNcLVuYrIamNMXHn76txQRQJCIUt7QymllCuaLIpoNZRSSpVKk0URbeBWqt6qK9XxZTnTc9RkUSQgFPIyoSDP05EopWpQQEAAqampdTphGGNITU0lICCgysfQ3lBFTs4PdQyCIj0bi1KqxrRo0YKkpCTOaH65s0BAQAAtWrgcqVAhmiyKOE/5oclCqXrD19eX2NhYT4dR62k1VBGdH0oppUqlyaKIzjyrlFKl0mRRRJOFUkqVSpNFEU0WSilVKk0WRQL0aXlKKVUaTRZF/IJAvHXKD6WUckGTRRERHcWtlFKl0GThTJOFUkq5pMnCmSYLpZRySZOFM00WSinlkiYLZw10mnKllHJFk4WzgFCd7kMppVzQZOFMq6GUUsolTRbOAkIhPxvysj0diVJK1SqaLJwVjeLOOebZOJRSqpbRZOFM54dSSimX3JosRGSkiGwTkUQRmehivb+ITHOsXykirR3L/UTkAxHZKCLrReQCd8Z5ks4PpZRSLrktWYiIN/AGMAroDFwnIp1LbHYbkGaMaQe8BDzrWH47gDGmGzAMeEFE3F8KKipZ6PxQSilVjDsvwP2ARGPMTmNMLjAVGFtim7HAR47X04GLRESwyWU+gDHmMHAUiHNjrJY+LU8ppVxyZ7JoDuxzep/kWOZyG2NMPpAORALrgbEi4iMisUAfIKbkB4jIBBGJF5H4annYelhL8PKF3zec+bGUUqoOcWeyEBfLTAW3mYxNLvHAy8AyIP+0DY2ZZIyJM8bERUdHn2G4gF8gtIiD3UvP/FhKKVWHuDNZJFG8NNACOFDaNiLiA4QCR4wx+caYe40xPY0xY4EwIMGNsZ7SejAcWAfZ2n1WKaWKuDNZrALai0isiPgB1wIzSmwzA7jZ8foqYIExxohIoIgEAYjIMCDfGLPZjbGe0nowmALYt7JGPk4ppc4GPu46sDEmX0TuBOYA3sBkY8xvIvIkEG+MmQG8D3wiIonAEWxCAWgEzBGRQmA/cKO74jxNi3623WL3Emg/rMY+VimlajO3JQsAY8wsYFaJZY85vc4Grnax326goztjK5VfIDTvo+0WSinlREdwu1LUbpGT4elIlFKqVtBk4UpRu8VebbdQSinQZOFajFO7hVJKKU0Wu1IyuWfqWrYcdOoq6xek7RZKKeWk3ieLrNwCvl13gN0pmcVXtB4MB9Zqu4VSSqHJgvAgXwDSTuQVX3Gy3WKFB6JSSqnaRZNFoB8AaSdyi6+I6W8nFlzyIhQWeiAypZSqPep9sgjw9SbA14ujJZOFXyCMeBr2LoP49z0TnFJK1RL1PlmALV2cVg0F0PMGaDsUfnoc0vbUfGBKKVVLaLIAwgL9Ti9ZAIjApa/Yf7+/G0zJSXOVUqp+0GQBRAT5ciTTRbIA+4yLi/8FOxfChmk1GVb9s3WmdihQqpbSZEFRycJFNVSRuNugaQ9Y/DwUFtRcYPXNzPthwX88HYVSygVNFkB4oO/pvaGceXnBoHsgNRG2zSp9O1V12ccg4wAkb/V0JEopFzRZYBu407PyKCwso03inMsgvDUsfVnbLtwhxfFsq8xkyEz1bCxKqdNossBWQxUaOJZdRlWUtw+c+zfYHw97ltVccPVFyvZTr5O3eC4OpZRLmiyw1VDgYhR3ST1vgMAo+OXlGoiqnknZdur1YU0WStU2miwoYxR3Sb4NoP+fIWEuHPqtBiKrR1ISIKoj+DfUdgulaiFNFkCYo2ThcqxFSX1vA79gmPUg5DttX1gIc/8Jy15zU5R1XPI2iO4A0Z3gsCYLpWobTRZARJAtWRzJLKcaCiAwAsa8BHuWwg/32MZuY2DmfTZR/Pws5GVXLZD8nKrtd7bLz4UjOyGqAzTqpG0WStVCmiywDdxQwZIFQPdxMGQirPsMlrwAPz0Kqz+wU4PkZsCO+ZUPYtPX8EwrSFpd+X3Pdmm77Ay/UR0h+hw4kQrHk2s2hsR5kHO8Zj9TqbOIJgugYYAP3l5SfpuFswsmQrerYcG/bYmi7+1w/RfQIMJe+CtrwxeQnwVf3gInjlR+/7NZsqNxO9pRsoCaLV0kb4dPr4QVb9XcZyp1ltFkAYgIYQ18y+8NVXwnuOx16HgJ9JsAo54Db1/ofBlsmw15WRU/Vk4G7FgAsUMg4yB899e6PZYjdQfkOj1sqqjbbGR7W7KAmm232LHA/rvz55r7TKXOMposHMICfSteDVXENwCumwKjn7ejvAG6XA55mZDwU8WPkzgPCnJgyEMw/N92lPjy1ysXy9kiOx3eGgQ/PXZqWcp2aNgC/IMhpIl9jkhNlix2LrT/Jv0KuScqv3/GITh2oHpjUqqW0WThEB7oR1pFGrjL02qwHYvx2zcV32fLD3aflgNs19xzLrXToidvL3/f8vxwHyz+X+0pqWz70Va3bfjyVOkreRtEtbevRWzpoqZKFgV59lnrEW2gIBf2Lq/c/jsWwut94bNx7olPqVrCrclCREaKyDYRSRSRiS7W+4vINMf6lSLS2rHcV0Q+EpGNIrJFRB52Z5xgG7kr1WZRGm8fWxW1/Ud7l3riiG2HmDzKdS+p/BzYPgc6jQYvb3uxvOQl8PKBlW+fWSz7frUPblrwb9sQfyZSEmDWA7ZkUJqje+GlbmWXqjZ/B95+kJNuk6Qx9tjRHU9tU9QjqiYSXNIqyD1uS3VePrBrUcX3jZ9s2zoKcuDQRi1dqDrNbclCRLyBN4BRQGfgOhHpXGKz24A0Y0w74CXgWcfyqwF/Y0w3oA9wR1EicZeIIN+yZ56tjC6XQ94JWPQMvHM+bPnePnFvoYsZVXcttj2oOl16allwNHS7CtZ/DllHqx7HL69AQBh0vdImjJWTqn6spS/Dr5NgyrWlV9VsngHpe+GrP7l+WFROhq1yi7vVTv2+9hM4tt9W20V1OLVd9DmQlQbHD1c93orasRDECzqMhBZ97fdREQv/Cz/ca3vA3fitXaZtHqoOc2fJoh+QaIzZaYzJBaYCY0tsMxb4yPF6OnCRiAhggCAR8QEaALnAMTfGSnigH0dO5GKq42621SAIamQv1uIFt82FPn+EZa/D7l+Kb7tlBviFQJshxZf3m2ATztpPqxZDSqJ9PkS/2+HySbYhfvYDsHF65Y9VkAdbf7AD5vYuhy9uKj4gsUjCHAhtaUsEX9x0eklq+xx7F97lcjt1yq5FNnlA8WRRmR5RruKojJ0LoXkfaBBmOxgcWGcTVVl+3wiLn4Pu18B1U231YVCjUw3lVZXxu01A+1ad2XGUcgN3JovmwD6n90mOZS63McbkA+lAJDZxZAIHgb3A/4wxp/UnFZEJIhIvIvHJyWfWLz8s0I/c/EKy8qrheRVe3jD0H/Y5GH9eYi9Gw/8D4a3g2/+zd9hgn42xdRZ0GA4+/sWP0awntBxo7+ar8gyN5a/Z6p5+E2zV2FWTIaY/zH6w8uMJdi2C7KNw0eNw6cuQ+BN8c4cdtV4kO91OsNj1Crj8bTi4Dn4sUfO4+VsIbgIt+kHP6wGxgxiheDVURXtEzf0nvNqr7KqxsmSlwf7V0OZC+77NEMDYNozSGAM/Pmwb4Uc+Y3+3ItDmAluycP6dVEZhIXzzZ1u19f7F8NXtkL6/asdSyg3cmSzExbKSt+2lbdMPKACaAbHA30WkzWkbGjPJGBNnjImLjo4+o2ArPJlgRfW5Bca8aC8qYHv6/OFtW68//VZY+hLM+xecSIFOY1wfo/8dcHSPvSMHe6e+dZYduFdWr53jh2Hd5/aCHNzILvMNgBFP2wFvq96t3Lls/s6WftoOted10WPw29ewffapbXYshMJ86DDCtr8MvtcOVFzhaHfJOW7bMjpfZnuOhbW0F+eMA7aqLMjp+wtuBA3C4fDm0mNKircltWNJVa9e27UETCG0dSSL5nHgGwg7y2i32DoTdi+BC/9hR/MXaXuhnV79cBXnDFv1ni3lDP8PnHe//Z2/2hNe6gpv9If3R8Dvm6p2bKWqgY8bj50ExDi9bwGUbAEs2ibJUeUUChwBrgd+NMbkAYdF5BcgDtjprmCLRnGnZebSPKyBez6k1UC48BFY+LSdjBDsRbH9MNfbdxoDDZvbbrRpu+CXV+H473adeEFEWwiMtKUS30B7AW7cxd4tF+TaKdWdtYiDdsNs9VjfP4F/SPkxF+TbhuiOI23CATj3boj/AJa/CZ0uscsS5tqLfot+9v2F/7QN1z8+ZEdnN2wG+dnQ2akmsud4ezce1cHenRcRsRfudZ/ZGIc8eCrpgq0W+/5uCGlqSyTLX4f+E4pvUxE7F9p5vlr0te99/KDVuaU3cufn2NJMdCdbreisqHSyYwE06Va5OJK321kA2g2DgXfa8+9zM/z6LmSm2Ab47XNgzccw+rnKHVupauLOksUqoL2IxIqIH3AtMKPENjOAmx2vrwIWGNtosBcYKlYQMABwa1/K8JOTCVZTyaI0Qx6ER5PhkYMwcS/ct7X0i7a3r524cM8vMOcRO8L5+i/hmk/h/AfshdLHz16E05Ns+8b3d8Gaj+xFPLLt6ce84GFb/bLyneLL87Jh3RSYdAG81udU4/LuJZB1pPhF3tvHlnr2LIWD620VSsJcaHeRXVe0zdUf2odGzXkE5vzDlh5aDjx1nHPG2GTp6uJ6+dvQ4zpY/oaN59d3Tw3kW/EWHNpkL5wX/8tWkVWldLFjIbQ+z/6ei8QOseM+XPVsWvGWTdojnj51nkUaNrXVZzsWVi6Ggjz4+nab7Me+fipphrW0Y24ufwuu+cRWcyXMKb2HWH4OfH4dPN8enmtjp46Z90TlYimSnmTPtcDN/xdU9ahq1Wclua1kYYzJF5E7gTmANzDZGPObiDwJxBtjZgDvA5+ISCK2RHGtY/c3gA+ATdiqqg+MMRvcFStAeFAFpymvDt6+xS9QZel7u33kaMfR0LL/qeXnXHr6toWFcHS3vVNtEef6eC36QPsRdoqSfhNs1cmaj23PpBOpdn6m9P0w9Xq4+XtbHeIbBO0uLn6c3jfBz8/Y0kV/x3Hajzj9PK+abHtHbf7W9oLy8j613rcB3LHYdYkgKMpePPveBrMnwizH87l7XGeTYcfRtuQlYl87ly4KC20JLKRp8RKLs+Tt9sI/4P+KLy/qaLDxSxh096nlaz+1Pco6jLRJ0ZW2F8Kq9+34Ed8Klk7XfGTbd8Z9bAcklqbDCJssUhLsTUNJcx6xgzm7X2urPFN3wNIXbam11bkVi6XI7Idsh4Z9K+GK905PjKr2MAa+vMmWdof+060f5da/AmPMLGBWiWWPOb3OxnaTLbnfcVfL3alS05TXpICGMKyCd4heXnZwWcRpzTvFXTAR3r0Q3jkP0naDeEPHUTZ5xDq6+n5xI3z7F9uVtMOI0y9+AaHQ60bb/uHjZ6vFSiYUsAnjyvch9jzXbTNhLcuOtVkvuPVH2LvCjjv5dRL4BNhR80WJYMhDMGmITV4NImDtx7ZtqEG4rWKKHWIHOzpf9Fa8Ad7+tmeWs8bdbEeAnx6DA2th1PO2PWHRM/bu/ooySjBtLoQVb9oeY20utBfcrKPQ+0bX2xcW2jv4Zr1tCaws7Yfbf7f/eHqy2PCljfHcv9k2D7ClsDcG2EGZdyy231FFJG+zcTftYQeWirc9Z+ckr2qPzd/Z/69F1b9upLcMDkUPQKrQNOVnu+a97R3ovhX2bqTneFuNUqTzZbZ6Z96/HO9L9nh26H+HvYCv+dheYIMiXW/n7WPbSKpKxLb3tBpoSz15WRDa4tT6Zj1t6WLFm/Z97BCb+JK32m6oCf+wDdmD7rLrMw6d3gGgiJcX3DLTPg1x0XN2nq/8bNvV99JXyi4Rth4EXr62im/+v+HAGru8eR9oXHKIEbbbcGqivXsvrQRUJCwGGnWx1X1F5wH24v793bZ676LHTy33C7IJ9fNrbGIcfG/Zxy/yy6vg0wDGf22/1/lP2MGKf3jz7E4YuSfAL9DTUVSvrDQ7ULZpDxjwF7d/nCYLB19vL0L8fWqmGqo2uOKdstcPuseWOrbPKb0BPiLWtjts+f7Una+7hZbsfe0w6jk73uGcy2xczqZca0sdXa+0+//6jusOAEW8fW2bUKcxtq2l1UDbQ6m8C7pfkI1h+492rqvR/7PTtix7zbY9lLTiTVtVVloyLqnDcHus7HRbssvLsuNZ/ALhqg9OT2QdR9pz+PlZ6HKF7brtbNdi+x33utGeW3oSbJhmqwuDouC8+2znhAX/sSWTS18t/3dQG+37FT4Ybav6Oo32dDTVZ+6jtup4/PQaqSrUuaGchAVVYTLBukrE3knfvcFeBEsz+D4IaXZ6dU5NC4uxbQwlEwXAqGfsRW/uP+wYl1Xv2TYfVx0AnDU6B2782iaOil4kh//HXlT/ttoOiOx9E2z84vQxE4e32N5Y/W6veBVR+xG2e3LR4L8F/7Glp8vfLl4ydDbqWVtFOOUaW0oyxlZRzfw7fHQpzPibnY4mN9N2JsDAuXee2v/8B2yiXPOxHTdTsoH96F677vt77OvaprDQ3n0X5tk2nLpi5yLbznju32zJogZoycJJeKBf9Y2zqCvKu5A17w1/r+VPtgtvDef9HRY+ZXv4ZKfbkpM7NOtpf4oM/IttZ1nxJox46tTyFW/ZtpeSXXDL0qKvbYfZPtcOblz+ht3fVVtRkdAWtpPB7Afh82ttz7PcTPtkwgF/tVPLzHsCjuywjeLdrj69HWnoP20pZsUbNlmFxthR7AfW2OMUOZFie+qVpyDfHqtRF2hfIva8LPsZZXXrzjhke+l1vbL8JL7uU9uBIHaI7RK971eIcXP9/pFd9vfTuZx2qCLG2PFUYa3KPp/k7XZA7O5fbKkwPNa2P9YQTRZOwgL9tGRRV517l+0avPUH2122RZ+a+dywlvaitvpDe5feIMyOndgwDXpcW3xgX3m8fWxiSJhr25vCYmz32vJ0dPTg2vglLHkRELj5B9vpAOxF+6vb7PQyzj3AiojYRJefZUtlYBNGk262t17bC+28YD8/DXtXFu+1V1JmCkz/o73YNYiAu9ed6g1XWAAfXmLr4v9vWek9yr6/y1b1pSTAhU5zjBpjS44BDe377HSY/yTEDIBrp8BLnW013jWflP87K03CPMCUXjV7cD18coVNnJe8UH5b3fHDtpS3ZYYt7V72uv0bKamo9GYKbJLoMhYG/q3ive6qQYWShYjcje3KmgG8B/QCJhpj5roxthoXHujL7pTM8jdUZx/fALjkf3Yq8fPuq9nPHnSXrYpa/LwdQLn6Q9tm0v//yt31NO1H2Iv+iRTbtbkiAyvBtmf0vN4xzUoJHYbDHYvsfGKNznG9vwhc8qJNDiFNTk9yYS3tDMc/PQq3znF9h3xgLUy70XazPv8BO3X+khdP9fZb/YEdUAp24krnRFAkabVNFGEtbQ+14Ea2e3XG7/Ziun02tDwXBvwZ9iy3yemG6bY7cdytdkDqkV2uqyud5efa31nReeSesN2TV39g3w++z5a4nBv9966Ez66230nsEJj1oL2wl+xqXVhoxy7tWGC7Kedm2g4nm6bbHopXfXCq67sxtrpxyf+g7UVw2avFO3fUoIqWLG41xrwiIiOAaOCP2ORRx5JFNU1TrmqndhfDxD0Vv8BWlybd7H/05a8DjnEh5/7t1ISJldHuIjuAr/fNtptzdalIl2sR1726wLZrXfiI7Zm19Yfi44ByM23PsuWv2wb9W3+0XaKLBv/1vc2e0/yzHRNFAAAgAElEQVQn7TkFRdvpcHpce/pFfeFTtkRyx2L4+g47/iZtF6z5xPZa6/snW/L64ia7fe+bTlUL9ptgSxYr37ZtOQV5dsDrwfV2HrLkrfZOPyvNzoQcFG0nBY3pZ+/sk7fakld2um3/OPSbLXEd2WmPsfQle343fWdLB5NH2vagP86CYwdtV+Rdi2xiM4753prH2Z5m0R1t+9X0P8L7w+2sBmEx9px2LbbnccmLFR+f5QYVTRZFtwmjsQPk1jtmh61TwgP9yMjOJ6+gEF9vbfuvk2o6URQZ9Sz89i10v9q2oVRVYITtdBBYSjdlT+o53g7SnPcvaNLdln5SEmDBU3bq+p7jbbVZUalk6D/tBXT+v23bWG6m7UHmH2J74f04Ea6fdur4e1fAjvlw8RO27ebqD+Hjy2wCaNEX/vCWfYhWYYFtzN8x3047U6RhM+h6lU0sBbl2jMKJVLsuuIm9YEd3sscOCLXtOLt/cUyA2Rhu/MbOjwb2/GY/CK/POXX8mP62zaaoO/Z1U+HdofD2YPveP9RWX4W3sscLjbFjmIpKJy3i4I4ltvSTst12GMhMsV2iB9/r8Z5oUpEpuUXkA+wMsbFAD+yI7J+NMTVU8Vu+uLg4Ex8ff0bH+Hj5bh777jdW/eNiokP8y91eKVXCttm2Id1ZdCcY85LrkeTznzz1YK5Bd8OwJ+3rX16xAyOvm2oHjAJ8dJmdXPLu9ad66GWn2wkhO46q2DiQgxvsM2Z8Auw+3a6yY1RKazsyxpaAGoTbqixnB9bC/jXQqLMtcbmaieDAOjtKv/1wm2hKzi5dC4jIamNMKVM+OG1XwWThBfQEdhpjjopIBNDC3VNwVEZ1JIsZ6w9w1+dr+ene82nf2EN3oEqdzYyB9VPtc0uCG9u77CbdS68+yT5mp5n39oM7V526IOfn2jvy1AQ751ZUO1sSGPE0DPzrmcWYvM1O0Fny4l9PVTRZVLQaaiCwzhiTKSLjgd7AK2cSYG1U7dOUK1XfiEDP6yq+fUBD2yDu7Vv84u3jBzd8aefkOrDWPmMkPNY2Up8p52enqAqraLJ4C+ghIj2AB7ETAH4MDClzr7NM0ZQf2sitVA2Kaud6eXgr+xAxsCUWY+x0LMojKvqbz3dMHT4WeMUY8wpQ5+ppooJtfeLhjBwPR6KUKkZEE4WHVbRkkSEiDwM3AueJiDfguT5cbtIoxB8/Hy/2HSnjKXRKKVUPVTRVXwPkYMdb/I7tGfW826LyEC8voWVEIHtSdWCeUko5q1CycCSIz4BQERkDZBtjPnZrZB7SKiKQPalaslBKKWcVShYiMg74FftAonHAShG5yp2BeUrLyED2HjlBRboUK6VUfVHRNot/AH2NMYcBRCQamAdMd1dgntIqIpATuQWkHM/VgXlKKeVQ0TYLr6JE4ZBaiX3PKq0i7cjQvUe03UIppYpU9IL/o4jMEZFbROQWYCYlnq1dV7SMtI9e1HYLpZQ6pULVUMaYB0TkSmAQdlLBScaYb9wamYe0CG+AiCYLpZRyVuGHHxljvgK+cmMstYK/jzfNQhuwV8daKKXUSWUmCxHJAFx1CxLAGGMauiUqD9OxFkopVVyZycIYU+em9KiIVpGBzNtyyNNhKKVUreHWHk0iMlJEtolIooic9mRxEfEXkWmO9StFpLVj+Q0iss7pp1BEerozVmctIwNJOZ7L8Zz8mvpIpZSq1dyWLBzzR70BjAI6A9eJSMlnMt4GpBlj2gEvAc8CGGM+M8b0NMb0xM5HtdsYs85dsZbUKsLRfVYbuZVSCnBvyaIfkGiM2WmMyQWmYmetdTYW+MjxejpwkYvHtV4HfO7GOE/TytF9VsdaKKWU5c5k0RzY5/Q+ybHM5TbGmHwgHSj5cOFrqOFkoWMtlFKqOHcmC1dPFy/Zs6rMbUSkP3DCGLPJ5QeITBCReBGJT05OrnqkJTQM8CU80Jc92n1WKaUA9yaLJCDG6X0L4EBp24iIDxAKHHFafy1llCqMMZOMMXHGmLjo6OhqCbpIy8ggbbNQSikHdyaLVUB7EYkVET/shX9GiW1mADc7Xl8FLHA8kQ8R8cLOcjvVjTGWqlVEIHu0zUIppQA3JgtHG8SdwBxgC/CFMeY3EXlSRC5zbPY+ECkiicB9gHP32vOBJGPMTnfFWJZWkYEcOJpNXkGhJz5eKaVqlQpP91EVxphZlJhw0BjzmNPrbGzpwdW+PwMD3BlfWVpGBFJQaNiflkXrqCBPhaGUUrVCnZxmvDoUTVWujdxKKaXJolStHd1ndyYf93AkSinleZosShEd4k9MRAOWJqR4OhSllPI4TRalEBEu6tSYpYkpZOUWeDocpZTyKE0WZbjonEbk5BfyS6KWLpRS9ZsmizL0j40k2N+H+VsPl7+xUkrVYZosyuDn48X5HaJYsPUQjrGCSilVL2myKMfQTo05dCyHTfuPeToUpZTyGE0W5biwYzQiMH+rPjlPKVV/abIoR2SwP71bhjN/i7ZbKKXqL00WFTC0UyM27k/n0LFsT4eilFIeocmiAi4+pzEAP23WqiilVP2kyaICOjQOpnPThkxavJPcfJ2FVilV/2iyqAAR4cGRHdl75ARTVu7xdDhKKVXjNFlU0JAO0ZzbNpJXFySSkZ3n6XCUUqpGabKoIBHhoZGdOJKZy7uLPfI8JqWU8hhNFpXQIyaMS7o35d0luzicoT2jlFL1hyaLSnpgeEfyCgp5/sdtng5FKaVqjCaLSmodFcSfzmvDl6uT9FkXSql6Q5NFFdxzcXtio4KY+PUGTuTmezocpZRyO00WVRDg682zV3YnKS2L/83Z7ulwlFLK7TRZVFG/2AhuHNCKD5btYvWeNE+Ho5RSbqXJ4gw8OLIjTRsGcPfUtRzJzPV0OEop5TaaLM5ASIAvb47vw+GMHP762RryCnQqEKVU3aTJ4gz1jAnjv5d3Y/nOVJ6aucXT4SillFu4NVmIyEgR2SYiiSIy0cV6fxGZ5li/UkRaO63rLiLLReQ3EdkoIgHujPVMXNmnBbcNjuXDZbt5Z9EO8rWEoZSqY9yWLETEG3gDGAV0Bq4Tkc4lNrsNSDPGtANeAp517OsDfAr82RjTBbgAqNUTMj08qhNDOzXiv7O3MuylxXy7dj8FhfrcbqVU3eDOkkU/INEYs9MYkwtMBcaW2GYs8JHj9XTgIhERYDiwwRizHsAYk2qMKXBjrGfMx9uL92+O450b++Dv48U909Yx7KVFTFm5l+y8Wh26UkqVy53Jojmwz+l9kmOZy22MMflAOhAJdACMiMwRkTUi8qCrDxCRCSISLyLxycnJ1X4ClSUijOjShFl3nccb1/cmyM+HR77ZyKBnFvD+0l2eDk8pparMx43HFhfLStbLlLaNDzAY6AucAOaLyGpjzPxiGxozCZgEEBcXV2vqfLy8hEu6N2V0tyas3HWENxYm8u8fNmOM4U/ntfF0eEopVWnuLFkkATFO71sAB0rbxtFOEQoccSxfZIxJMcacAGYBvd0Yq1uICAPaRPLhH/sxulsT/jNzC9+t2+9y22/X7mfo/35m+uokjKk1eU8ppQD3JotVQHsRiRURP+BaYEaJbWYANzteXwUsMPZKOQfoLiKBjiQyBNjsxljdyttLeHFcT/rHRnD/l+v5JbH4BITLd6TywPT1HM7I4f4v13PduytIPHzcQ9EqpdTp3JYsHG0Qd2Iv/FuAL4wxv4nIkyJymWOz94FIEUkE7gMmOvZNA17EJpx1wBpjzEx3xVoTAny9mXRTHG2igvnjB6v435xtnMjNZ2fycf786WpaRgSy9KELefrybmw+cIzRryxh2qq9ng5bKaUAkLpS5REXF2fi4+M9HUa5Uo7n8NTMLXyzdj9NQwPw8RZO5BTwzV8G0TIyEIDkjBzu+2IdSxJSuHFAKx4d0xk/Hx0/qZSqfo724LjyttMrUA2LCvbnpWt6Mv3PAwkP9OPwsRwm3dTnZKIAiA7x54Nb+jLh/DZ8smIP499bycakdA9GrZSq77Rk4UEFhYbj2fmEBvqWus136/bzyNcbycwtIK5VOLcMas1FnRrTwM+7BiNVStVVFS1ZaLI4CxzLzuPL+CQ+WrabvUdO4OftRVzrcAa3j6JnTBidmjQkIsjP02Eqpc5CmizqoIJCw7IdKSzensyShBS2/p5xcl2jEH+6NQ+lR0wYPWLC6B8bQYCvlj6UUmWraLJw56A8Vc28vYTz2kdzXvtowDaWbzl4jK0HM9hy8Bgb9qezYNthjIE2UUE8f3UP+rQK93DUSqm6QEsWdUxGdh7LdqTy5PebOZiexe3nt2Fox0ZsP3ycxEMZdGkWytVxLbBTcCml6juthqrnMrLzeGrmFqauOjU9l5+PF7n5hYzs0oRnr+pOaIPSG9aVUvWDJgsFwJq9aaSfyKNDkxCaNgzgvaU7ee7HbTQNC+C163rTMybM0yEqpTxIk4Uq1eo9adz1+VoOpGdxQ/+W3D+8I2GB2ptKqfpIB+WpUvVpFc7se87j5oGtmbJyL0NfWMTHy3dzPCe/1H3yCgrZcvAY6Vm1+hlUSik30ZJFPbf5wDEe+24T8XvSaODrzZjuTbm2Xwy9W4afbATfkXyce6auY+N+O4q8UYg/fVqF89xV3QkJONXukXj4ODe+v5JXru1Fv9gIj5yPUqpytOusqpDOzRry5Z8Hsm7fUaat2seM9Qf4cnUSnZqEcEP/logIT83cgr+vF0+O7UJmTgEJhzL4eu1+OjQO4d5hHU4e6+V52zmYns2/f9jMd38dhJeX9rhSqq7QZKEQEXq1DKdXy3AeHdOZGesP8OmKPTz63W8ADG4XxQvjetC4YcDJfbLzC3hvyU5uPrc1EUF+bD+UwcyNB+nctCEb96cza9NBxnRvVuWYlu9IxWA4t23UGZ+fUurMaZuFKibI34fr+rXkh78N5tu/DuKdG/vw8a39iiUKgPuGdSArr4C3fk4E4JX5CQT5+fDJbf3o2DiE5+dsI6+gsEoxGGP4+xfr+NuUtfr8cqVqCU0WyiURoWdMGCO6NHFZndSuUQiX92rBR8v3sGh7MrM2HuSWc1sTGezPQ6M6sif1BFN/rdrzONYnpXMgPZvUzFy+Xev6yYJKqZqlyUJV2T0Xt8cYw+0fxxPk58Ntg2MBuLBjI/rFRvDK/AQysivfe2r2xoP4egvtGgXz3tJd+phZpWoBTRaqymIiArmuX0ty8wu55dzWhDtmvhURJo7qRGpmLhe9sIgPftlV4eokYwyzN/3OuW2j+MsFbUk8fJxF25PdeRpKqQrQZKHOyD0Xd+BPg2OZMKRNseW9W4YzbcJAYqOCeOL7zQx5fiFv/byDI5m5ZR5v88Fj7D1yglFdmzCmezMahfjz/tJd7jwFpVQFaLJQZyQiyI9/julMw4DT55nqFxvBtDsGMuX2/rSJCubZH7cy4L/zuW/aOnYkH3d5vNkbf8dLYFjnxvj5eHHzua0d07EfY//RLD78ZRdTVtb9Z5Ov33eUMa8tYcvBY54ORSlAu86qGnBu2yjObRtFwqEMPlmxh69WJzFr00H+cUlnxjvGchSZvekgA9pEEhnsD8AN/Vvy+oJErnlnRbHR495ecE3fljV+LjVhb+oJbvtoFSnHc/lkxR6evrybp0NSSksWqua0bxzCk2O7suD+C+jbOoJHv93EbR/Fc+BoFgAJhzLYkZzJqK5NTu4TFujHXRe1p2PjECaO6sRP957Pee2j+Oe3m1i9J81Tp+I2aZm53PLBr+QXGvq1jmD2xoNV7oKsVHXS6T6URxQWGj5evpunZ28lv6CQCzo2ooGvN7M2HWTlwxfRqMS4DmdHT+Ry2eu/kJVXwPd3DqZJaOnb1oQX5m4j5XgOT1/erVgpKTuvgKMn8iocX3ZeATe8t5KN+9OZ8qf+pJ3I4/aP4/nglr5c2KlRlePbkXwcY6Bdo+AqH0PVXTqRoKrVvLyEWwbFMv++IfzfBW3ZtD+dmRsPEtcqvMxEAba08e5NcWTm5DP+/ZV8smIPyRk5NRR5cev3HeW1BYl8/uu+Yg3xWbkFXDtpBRe/uIjDx7IrdKz/ztrC6j1pvDSuJ3GtIzi/QxQNA3z4fv2BKsdXWGi49cNV3PbRKgoL68aNofIMbbNQHhUTEcgDIzpx78Ud+GVHKq0jAyu0X8cmIbx5Q2+e/GEzj367ice+20SnJg0RILegkNx8x09BIX7eXtzQvyV/HBxLsH/1/ckbY3jyh81EBfvRo0UYz8zeSlzrCLo1D+XuqWtZn3QUbxFempfAf68ou91hwdZDfLR8D7cOiuWS7k0B8PfxZlTXpvyw4QDZeQVVeqb64oRk9qSeAODX3UcY0Cay8ieqFG4uWYjISBHZJiKJIjLRxXp/EZnmWL9SRFo7lrcWkSwRWef4edudcSrP8/H2YkiHaFpFBlV4nws6NmL+fUOYc8/5/G1oexqF+NMsLIAOjYPp3TKM8ztEMbpbE85pGsILP23n/OcW8u7inWXe6ecVFPJl/D72pGaW+/nfbzjI6j1p3D+8Iy9e05PGDQO4c8oa/vntRuZuPsTjYzozfkArpq3aS+LhjFKPczgjm/u/3MA5TRvy0KiOxdZd1rMZmbkFLNh6uMK/F2efrthDVLAfIf4+fBG/r/wdlCqF20oWIuINvAEMA5KAVSIywxiz2Wmz24A0Y0w7EbkWeBa4xrFuhzGmp7viU3WDiNCxSQgdm4SUud26fUd5Ye42npq1hadmbaFz04Zc2CmaEV2a0K15KCLC1t+Pcf+X69m0/xihDXx5e3wfBrZ1fSeenVfAM47jXB0Xg7eX8Nr1vRj39nI+/3Uffxocyy2DYjmSmctXq5N4ZvZW3ru572nHKSw03P/lBjJz8nn12p74+xQvPQxoE0l0iD8z1h1gdLemlfrdJKWdYMHWw/zlgnYcOZHL12uS+NdlXVx2c1aqPO4sWfQDEo0xO40xucBUYGyJbcYCHzleTwcuEucWQqWqSc+YMD65rT+z7z6PB0Z0JNjfh7cX7eSy13/h/OcXcu+0dVz62lIOHs3m6cu7ER3iz43vr3R5N56dV8Azs7dyID2bxy7tjLdj7qzeLcN5/uru3HF+Gx4ZfQ5gx6H85cJ2zNtymBU7U0871uRfdrF4ezKPjulM+8anJzxvL+GSbk1ZsO0w8zYf4o2Fidz1+VrW7C2/J9jnjrm5ruvfknFxMWTnFfLD+oOV+r0pVcRtvaFE5CpgpDHmT473NwL9jTF3Om2zybFNkuP9DqA/EAz8BmwHjgH/NMYscfEZE4AJAC1btuyzZ88et5yLqpuOnshl7uZDzNxwkOU7UxneuTFPju1KRJAf6Vl53DllDUsSUhjQJoJ+rSPo1TKctXvT+HTlXo5k5nJl7xa8MK5HuZ+TnVfA0P/9TESwH1/cMZBAP1ugX7/vKFe9vYyhnRrx9vg+lHaftGZvGle8uezke38fL0Ib+DL77vNOjkcpKSe/gEHPLKBnTDjv3RyHMYYRLy8m0M+Hb/86qAq/LVVX1YaHH7n6yy+ZmUrb5iDQ0hiTKiJ9gG9FpIsxpthwVmPMJGAS2K6z1RCzqkfCAv0YFxfDuLgYjDHFLtahDXyZfEtfXl+QyLwth3h9YSKFBkTgok6NuW1wLAPaVOxpgAG+3jx2aWf+8tkarnt3Je/fHIe/jxd/+3wtjUICeO7KHqUmCrAllkk39iE4wIcuzULZn5bFH978hXu/WM+Ht/R1OSvwj5t+J+V4LjcObAXY6rpxcTH8Z+YWth/KoIOLUoxSZXFnskgCYpzetwBK9gEs2iZJRHyAUOCIscWdHABjzGpHiaMDoAMplFu4ulj7entx77AO3DusA5k5+WxISqdZWEClGuGLjOzalLfH9+GuqWu54s1ltGsUzP6jWUybMIDQwPLbEIZ3OTVQMbSBL4+N6cw/v93E24t38JcL2hXbduvvx3h5XgKtIgM5r92ph0dd3qs5z8zeysvztjO6W1P8fbxp3yiY1lFln09ufiF+Pmd3L/uCQnOyulBVjTv/AlYB7UUkVkT8gGuBGSW2mQHc7Hh9FbDAGGNEJNrRQI6ItAHaAzvdGKtSZQry92Fg28gqJYoiw7s04fPbB3A8J58FWw9z37AOxLWu2rPKb+jfkku6N+WFudt56+cdrN2bRlZuAa8vSODS15aSkZ3H05d3K1bqiAz2Z3S3psza+Dt3TlnL7R/HM+LlxSzfcXpbSpHPVu6h+xNzmL3x7G3r2Jl8nJ5PzuXV+Qk63f0ZcOsIbhEZDbwMeAOTjTFPiciTQLwxZoaIBACfAL2AI8C1xpidInIl8CSQDxQAjxtjvi/rs3QEtzpb7E09weKEZK7r1/KM7naPZecx/r2VbEhKB2wVmTEwpnvTk20vJeUXFHLgaDY5+QUcz8nnwekbOHA0iym3D6BHTFixbXenZDLylcUUFkKBMbw4rgdjezYHYEPSUWZuPMitg2JPe4pibfPST9t5ZX4CAH8aHMs/LjmnzGq/+qaibRY63YdSZ7nf07NZuzeNDfvTTz7dsKIOHcvmqreXkZGdz7QJA092QS4oNFzzznK2H8rgm78O4pGvN/Lr7iPce3EHVu9JO/mMkbbRQUydMJDoENcN7bXB8JcWEdrAly7NQvlw2W6uiYvh6Su6abWUg073oVQ90SQ0gFHdmvLQyE6VShQAjRsG8NltA/D38eLqt5fx6vwE0rPymLx0F/F70vjXZV1oGx3Mh3/sx+B2Ubz403Y27k/nwZEdmXxLHPuPZjH+vZXlPqfEUxIPZ7D90HEu6daUxy/tzF1D2zEtfh/P/bjV06GddbRkoZRiV0om//lhM/O3HibY34fcgkKGdIhm0o2nuvRm5xWwJCGFQe0iT3b/XZqQwq0fraJ9o2CevbI7XZo1rFVVPK/OT+DFn7az8pGLTlaX/eObjXy2ci+Tb4ljaKfGHo7Q87QaSilVaZsPHOONnxPZ9nsGU27vT6OQ8tsjFm47zJ8/WU1OfiHNwxowsmsTxg9oRWw5vaxqwsiXFxPs78P0/zv35LLsvAIuf3MZB9OzmHXXeTQLa+DBCD1Pk4VSqsYcycxl3pZD/Ljpd5YmpFBgDFf0as5dF7UnJqJik0MeOpbNhqR0wgN9q9xLzNnO5OMMfWERj47pzG2DY4ut25WSyZhXl9CpaUNuGxzL4u3JrNiZSvPwBlzSrRkjuzZx2UGgSG5+IUsSkklKy6JxQ38aNwygXaNgQs7CqVQ0WSilPCI5I4e3ft7Bpyv3UFhoOL9DNEMcPy0jAvHyEowx7E49wS+JKSzfkcrqPWn87jTB43nto3hoZCe6Ng8t9XMKCw2rdh/htwPHOLddJB0bhxSrAntjYSLPz9nGsolDXZYevlu3n7unrgMgJMCH/rGR7Ew+zs6UTLy9hBsHtOLRMZ2LNYRvP5TBJ8v38MOGA6SdyCt2vLBAXz76Y7/TepXVdposlFIedehYNu8t2clPmw+x2zFNOoCXgI+XF7mOJwA2DQ2gX2wEPVqE0b1FKOv2HeX1hYkcPZHHyC5NuLJPC4Z0iMbPx4vc/EI27k9n4dbDfLN2P/sdT1kEaBUZyPDOjRnULoq+rSMY985y/Hy8+OYvpU9vMn/LIUIb+NIzJgwfby+MMWw5aB//+/mvexneuTGvXtcLP28v3lu6k+fnbMNLhOFdmnB5r2Z0bR7K4WM5HDiaxX9mbuFIZi7v3xxH/7NoKnhNFkqpWmN3SiZLElM4cjyX/MJC8goMzcMbMKhtJLFRQac1ih/LzuOdRTv4/Nd9HMnMJbSBL+0aBbNpfzo5+YV4CQxuH80VvZrTu2U4SxKTmfPbIZbvSCGvwODjJeQXGv4x+hxuP79NlWL+8JddPPHDZnrFhBHo58PSxBRGdGnMf6/o7rKK6vf0bMa/v5KktBO8dUOfKj3dMCu3gA1JR+nWIvRkJwKwz05ZmphCs7AGtI2u3icearJQSp318goKWZqYwndr97MvLYteMWHEtY6gb+twl5MonsjNJ353Gst3prIz+ThPXd6NqFImW6yI2RsPcve0dXiL8Pilnbmmb0yZvb1Sj+dw0+Rf+e3AMXrEhHFNXAyX9mhaZlvGsew84ncf4fv1B5n72+9k5hbQPKwBj1/amWGdG7MzJZNHvt7Iyl1HALj4nMbcfl4sgX4+rNiZyspdqXRuFsp9wzpU6Rw1WSilVDVIPHycAF8vWoRXrKH+eE4+01bt44tV+9h2KIMGvt6M7taUa/vFENcqnD2pJ1ixM5Vfdx9h/b6j7Ei2D9pqGODD6G5N6RcbwTuLdrLtUAZxrcLZsD+dAB8vHhjZiZSMHD5evrtYe0lsVBBX9m7OnUPbV+n8NFkopZQHGWNYt+8oX8TvY8a6A2TmFhDs78PxnHwAooL96BkTTs+YUHrEhNEvNuLkw6/yCgr58JfdvDo/gfM7RvP4pZ1PdmPOyi1g5saD+HoLA9pEnvF0K5oslFKqlsjMyWfmxoOs3p1G1xahDGwTSdvo09tqSiosNC6noK9OteF5FkoppbCzFhc9O6Uy3J0oKkPnhlJKKVUuTRZKKaXKpclCKaVUuTRZKKWUKpcmC6WUUuXSZKGUUqpcmiyUUkqVS5OFUkqpctWZEdwikgzsOYNDRAEp1RTO2aI+njPUz/PWc64/KnverYwx0eVtVGeSxZkSkfiKDHmvS+rjOUP9PG895/rDXeet1VBKKaXKpclCKaVUuTRZnDLJ0wF4QH08Z6if563nXH+45by1zUIppVS5tGShlFKqXJoslFJKlaveJwsRGSki20QkUUQmejoedxCRGBFZKCJbROQ3EbnbsTxCRH4SkQTHv+GejtUdRMRbRNaKyA+O97EistJx3tNExM/TMVYnEQkTkekistXxnQ+sD9+1iNzr+PveJCKfi0hAXfyuRWSyiBwWkU1Oy1x+v2K96ri+bRCR3lX93HqdLETEG3gDGAV0Bq4TkZG+8yoAAAToSURBVM6ejcot8oG/G2POAQYAf3Wc50RgvjGmPTDf8b4uuhvY4vT+WeAlx3mnAbd5JCr3eQX40RjTCeiBPfc6/V2LSHPgLiDOGNMV8AaupW5+1x8CI0ssK+37HQW0d/xMAN6q6ofW62QB9AMSjTE7jTG5wFRgrIdjqnbGmIPGmDWO1xnYi0dz7Ll+5NjsI+APnonQfUSkBXAJ8J7jvQBDgemOTerUeYtIQ+B84H0AY0yuMeYo9eC7xj4muoGI+ACBwEHq4HdtjFkMHCmxuLTvdyzwsbFWAGEi0rQqn1vfk0VzYJ/T+yTHsjpLRFoDvYCVQGNjzEGwCQVo5LnI3OZl4EGg0PE+EjhqjMl3vK9r33kbIBn4wFH19p6IBFHHv2tjzH7gf8BebJJIB1ZTt79rZ6V9v9V2javvycLV09DrbF9iEQkGvgLuMcYc83Q87iYiY4DDxpjVzotdbFqXvnMfoDfwljGmF5BJHatycsVRRz8WiAWaAUHYKpiS6tJ3XRHV9vde35NFEhDj9L4FcMBDsbiViPhiE8VnxpivHYsPFRVJHf8e9lR8bjIIuExEdmOrGIdiSxphjqoKqHvfeRKQZIxZ6Xg/HZs86vp3fTGwyxiTbIzJA74GzqVuf9fOSvt+q+0aV9+TxSqgvaPHhB+2QWyGh2Oqdo56+veBLcaYF51WzQBudry+GfiupmNzJ2PMw8aYFsaY1tjvdoEx5gZgIXCVY7M6dd7GmN+BfSLS0bHoImAzdfy7xlY//X979xNiYxTGcfz7Q2QakWKj0LCRYrCZQk3ZycLCNMWgKTsbCyUiUZZ2lNmokUkoI0uZxWQWGsyMlKXVbKw0NYk0HotzLmNizp0xf253fp/dfXvvue/buW/Pec/7nudpkdSQ/++V867bvp7iX/37DDiV34pqAcYq01UzteRXcEs6TBptLgfuRsSNRT6kOSfpAPASeM/vuftLpOcWj4DNpIutLSKmPjirC5JagfMRcURSE+lOYz0wDHRExLfFPL65JKmZ9EB/JfAR6CQNDOu6ryVdA9pJb/8NA2dI8/N11deSHgCtpFTkn4CrwFP+0r85cN4ivT31BeiMiDez+t2lHizMzKxsqU9DmZlZFRwszMysyMHCzMyKHCzMzKzIwcLMzIocLMxqgKTWSlZcs1rkYGFmZkUOFmYzIKlD0qCkEUlduVbGuKSbkoYk9UnakPdtlvQq1xHonVRjYLukF5Le5e9sy803TqpD0ZMXVJnVBAcLsypJ2kFaIbw/IpqBCeAEKWndUETsBfpJK2oB7gEXImIXafV8ZXsPcDsidpPyF1XSL+wBzpFqqzSRcluZ1YQV5V3MLDsE7ANe50H/alLCth/Aw7zPfeCJpLXAuojoz9u7gceS1gCbIqIXICK+AuT2BiNiNH8eAbYCA/N/WmZlDhZm1RPQHREX/9goXZmy33Q5dKabWpqcs2gCX59WQzwNZVa9PuCYpI3wq+7xFtJ1VMlsehwYiIgx4LOkg3n7SaA/1xEZlXQ0t7FKUsOCnoXZLHjkYlaliPgg6TLwXNIy4DtwllRgaKekt6QKbe35K6eBOzkYVLK/QgocXZKu5zbaFvA0zGbFWWfN/pOk8YhoXOzjMJtPnoYyM7Mi31mYmVmR7yzMzKzIwcLMzIocLMzMrMjBwszMihwszMys6CdlvmfknkfNTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'frameId': Id_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1068, 1080, 1092, 3660, 3684, 3696, 3708, 3720, 3732, 3744, 3756, 3780, 3792, 3804, 3816, 3828, 3840, 3852, 3864, 3876, 3888, 3900, 3936, 6420, 7440, 7464]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.08\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "pred_frameIds = []\n",
    "for y,frameId in zip(y_pred, Id_test):\n",
    "    if y==1:\n",
    "        pred_frameIds.append(frameId)\n",
    "pred_frameIds.sort()\n",
    "print(pred_frameIds)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1201, 34, 60)\n",
      "1201\n",
      "(1201, 34, 60)\n",
      "(961, 34, 60, 1)\n",
      "(240, 34, 60, 1)\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "print(grids.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = grids.reshape((len(grids), grids.shape[1], grids.shape[2], 1))[:train_end_ind], grids.reshape((len(grids), grids.shape[1], grids.shape[2], 1))[train_end_ind:]\n",
    "Id_train, Id_test = frameIds[:train_end_ind], frameIds[train_end_ind:]\n",
    "\n",
    "print(len(grids))\n",
    "\n",
    "#X_train, X_test, Id_train, Id_test = train_test_split(grids.reshape((len(grids), np.prod(grids.shape[1:]))), frameIds, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "print(grids.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(34*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "input_dim = (grids.shape[1], grids.shape[2], 1)\n",
    "\n",
    "print(input_dim)\n",
    "\n",
    "\n",
    "input_img = Input(shape=input_dim)  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (4, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 961 samples, validate on 240 samples\n",
      "Epoch 1/50\n",
      "961/961 [==============================] - 9s 9ms/step - loss: 0.5010 - val_loss: 0.4099\n",
      "Epoch 2/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.2544 - val_loss: 0.2375\n",
      "Epoch 3/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.1710 - val_loss: 0.2045\n",
      "Epoch 4/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.1535 - val_loss: 0.1880\n",
      "Epoch 5/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.1415 - val_loss: 0.1786\n",
      "Epoch 6/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.1324 - val_loss: 0.1662\n",
      "Epoch 7/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1252 - val_loss: 0.1587\n",
      "Epoch 8/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1194 - val_loss: 0.1523\n",
      "Epoch 9/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1144 - val_loss: 0.1463\n",
      "Epoch 10/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1105 - val_loss: 0.1435\n",
      "Epoch 11/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1078 - val_loss: 0.1433\n",
      "Epoch 12/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1059 - val_loss: 0.1398\n",
      "Epoch 13/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1047 - val_loss: 0.1425\n",
      "Epoch 14/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1035 - val_loss: 0.1349\n",
      "Epoch 15/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.1008 - val_loss: 0.1323\n",
      "Epoch 16/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0994 - val_loss: 0.1304\n",
      "Epoch 17/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0984 - val_loss: 0.1319\n",
      "Epoch 18/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0972 - val_loss: 0.1279\n",
      "Epoch 19/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0959 - val_loss: 0.1291\n",
      "Epoch 20/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0953 - val_loss: 0.1256\n",
      "Epoch 21/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0936 - val_loss: 0.1248\n",
      "Epoch 22/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0928 - val_loss: 0.1258\n",
      "Epoch 23/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0919 - val_loss: 0.1228\n",
      "Epoch 24/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0909 - val_loss: 0.1246\n",
      "Epoch 25/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0900 - val_loss: 0.1220\n",
      "Epoch 26/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0890 - val_loss: 0.1207\n",
      "Epoch 27/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0883 - val_loss: 0.1372\n",
      "Epoch 28/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0901 - val_loss: 0.1190\n",
      "Epoch 29/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.0868 - val_loss: 0.1200\n",
      "Epoch 30/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0864 - val_loss: 0.1197\n",
      "Epoch 31/50\n",
      "961/961 [==============================] - 7s 8ms/step - loss: 0.0858 - val_loss: 0.1175\n",
      "Epoch 32/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0848 - val_loss: 0.1183\n",
      "Epoch 33/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0846 - val_loss: 0.1171\n",
      "Epoch 34/50\n",
      "961/961 [==============================] - 7s 8ms/step - loss: 0.0836 - val_loss: 0.1218\n",
      "Epoch 35/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0838 - val_loss: 0.1156\n",
      "Epoch 36/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0826 - val_loss: 0.1199\n",
      "Epoch 37/50\n",
      "961/961 [==============================] - 6s 7ms/step - loss: 0.0825 - val_loss: 0.1143\n",
      "Epoch 38/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0812 - val_loss: 0.1134\n",
      "Epoch 39/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0806 - val_loss: 0.1130\n",
      "Epoch 40/50\n",
      "961/961 [==============================] - 7s 8ms/step - loss: 0.0802 - val_loss: 0.1135\n",
      "Epoch 41/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0797 - val_loss: 0.1149\n",
      "Epoch 42/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0799 - val_loss: 0.1141\n",
      "Epoch 43/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0791 - val_loss: 0.1112\n",
      "Epoch 44/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0784 - val_loss: 0.1112\n",
      "Epoch 45/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0778 - val_loss: 0.1124\n",
      "Epoch 46/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0775 - val_loss: 0.1104\n",
      "Epoch 47/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0769 - val_loss: 0.1102\n",
      "Epoch 48/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0764 - val_loss: 0.1091\n",
      "Epoch 49/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0759 - val_loss: 0.1148\n",
      "Epoch 50/50\n",
      "961/961 [==============================] - 7s 7ms/step - loss: 0.0767 - val_loss: 0.1088\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"models/model_2D_conv.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[checkpointer]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('models/model_2D_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        (None, 34, 60, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 34, 60, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 17, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 17, 30, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 9, 15, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 9, 15, 8)          584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 5, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling (None, 10, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 10, 16, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling (None, 20, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 17, 30, 16)        1552      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling (None, 34, 60, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 34, 60, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,769\n",
      "Trainable params: 4,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=(1,2))\n",
    "print(mse.shape)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse.reshape((len(mse),)),\n",
    "                        'frameId': Id_test})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12264, 12480, 12492, 12540, 13080, 13164, 13200, 13212, 13236, 13248, 13272, 13296, 13308]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "pred_frameIds = []\n",
    "for y,frameId in zip(y_pred, Id_test):\n",
    "    if y==1:\n",
    "        pred_frameIds.append(frameId)\n",
    "pred_frameIds.sort()\n",
    "print(pred_frameIds)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
