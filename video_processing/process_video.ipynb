{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52d3d61c31dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprep_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_to_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletterbox_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image, letterbox_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim\n",
    "\n",
    "def write(x, img):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = random.choice(colors)\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"video.avi\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"cfg/yolov3.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"yolov3.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
    "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
    "                        default = \"416\", type = str)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--video VIDEO] [--dataset DATASET]\n",
      "                             [--confidence CONFIDENCE]\n",
      "                             [--nms_thresh NMS_THRESH] [--cfg CFGFILE]\n",
      "                             [--weights WEIGHTSFILE] [--reso RESO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/Eren/Library/Jupyter/runtime/kernel-16db8a81-c358-43b1-a8db-14ac9259388c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "confidence = float(0.7)\n",
    "nms_thesh = float(0.4)\n",
    "start = 0\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "bbox_attrs = 5 + num_classes\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(args.cfgfile)\n",
    "model.load_weights(args.weightsfile)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = args.reso\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model(get_test_input(inp_dim, CUDA), CUDA)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "videofile = args.video\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frames = 0\n",
    "start = time.time()    \n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "\n",
    "        img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "\n",
    "        im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
    "\n",
    "\n",
    "        if CUDA:\n",
    "            im_dim = im_dim.cuda()\n",
    "            img = img.cuda()\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            output = model(Variable(img), CUDA)\n",
    "        output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "\n",
    "        if type(output) == int:\n",
    "            frames += 1\n",
    "            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "            cv2.imshow(\"frame\", orig_im)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        im_dim = im_dim.repeat(output.size(0), 1)\n",
    "        scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
    "\n",
    "        output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "        output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "\n",
    "        output[:,1:5] /= scaling_factor\n",
    "\n",
    "        for i in range(output.shape[0]):\n",
    "            output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "            output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "\n",
    "        classes = load_classes('data/coco.names')\n",
    "        colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "        list(map(lambda x: write(x, orig_im), output))\n",
    "\n",
    "\n",
    "        cv2.imshow(\"frame\", orig_im)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "        frames += 1\n",
    "        print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Some code sourced from:\n",
    "\n",
    "#############################################\n",
    "# Object detection - YOLO - OpenCV\n",
    "# Author : Arun Ponnusamy   (July 16, 2018)\n",
    "# Website : http://www.arunponnusamy.com\n",
    "############################################\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def get_yolo_output(image):\n",
    "\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    scale = 0.00392\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    boxes_nms = []\n",
    "    class_ids_nms = []\n",
    "    confidences_nms = []\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        boxes_nms.append(boxes[i])\n",
    "        class_ids_nms.append(class_ids[i])\n",
    "        confidences_nms.append(confidences[i])\n",
    "    \n",
    "    return boxes_nms, class_ids_nms, confidences_nms\n",
    "\n",
    "\n",
    "def print_on_image(image, boxes_nms, class_ids_nms, confidences_nms):\n",
    "\n",
    "    cv2.polylines(image,swalk_pts,True,(0,255,255),thickness=3)\n",
    "    cv2.polylines(image,lane_pts,True,(255,0,255),thickness=3)\n",
    "    \n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_prediction(image, class_ids_nms[i], confidences_nms[i], round(x), round(y), round(x+w), round(y+h))\n",
    "        \n",
    "\n",
    "        \n",
    "def create_grid_image(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor, grid_background):\n",
    "    \n",
    "    \n",
    "    grid_vehicles   = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_person     = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_bicycle    = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "\n",
    "        x_plus_w = round(x+w)\n",
    "        y_plus_h = round(y+h)\n",
    "\n",
    "\n",
    "        x = round(x/grid_scaling_factor)\n",
    "        y = round(y/grid_scaling_factor)\n",
    "        x_plus_w = round(x_plus_w/grid_scaling_factor)\n",
    "        y_plus_h = round(y_plus_h/grid_scaling_factor)\n",
    "        \n",
    "        if class_ids_nms[i] in [2, 3, 5, 6, 7]:  # Motorized vehicles [car, motorcycle, bus, train, truck]\n",
    "            \n",
    "            cv2.rectangle(grid_vehicles, (x,y), (x_plus_w,y_plus_h), vehicle_w, -1)\n",
    "\n",
    "\n",
    "        elif class_ids_nms[i]==0:  # Person\n",
    "            \n",
    "            cv2.rectangle(grid_person, (x,y), (x_plus_w,y_plus_h), person_w, -1) \n",
    "            \n",
    "        elif class_ids_nms[i]==1:  # Bicycle\n",
    "            \n",
    "            cv2.rectangle(grid_bicycle, (x,y), (x_plus_w,y_plus_h), bicycle_w, -1)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    grid_overflow = np.zeros((grid_y_size,grid_x_size), dtype = np.int16)    \n",
    "    grid_overflow = np.array([grid_background,grid_vehicles,grid_person,grid_bicycle])\n",
    "    grid_overflow = grid_overflow.sum(axis=0)\n",
    "    grid_overflow[grid_overflow>127] = 127\n",
    "    grid_overflow[grid_overflow<-128] = -128\n",
    "    grid = np.array(grid_overflow,dtype=np.int8)\n",
    "\n",
    "    return grid\n",
    "       \n",
    "\n",
    "    \n",
    "def create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor, grid_background):\n",
    "    \n",
    "    \n",
    "    grid_vehicles   = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    grid_person     = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    grid_bicycle    = np.zeros((grid_y_size,grid_x_size), dtype = bool)\n",
    "    \n",
    "    present_val = 1\n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "\n",
    "        x_plus_w = round(x+w)\n",
    "        y_plus_h = round(y+h)\n",
    "\n",
    "\n",
    "        x = round(x/grid_scaling_factor)\n",
    "        y = round(y/grid_scaling_factor)\n",
    "        x_plus_w = round(x_plus_w/grid_scaling_factor)\n",
    "        y_plus_h = round(y_plus_h/grid_scaling_factor)\n",
    "        \n",
    "        \n",
    "        if class_ids_nms[i] in [2, 3, 5, 6, 7]:  # Motorized vehicles [car, motorcycle, bus, train, truck]\n",
    "            \n",
    "            grid_vehicles[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "\n",
    "        elif class_ids_nms[i]==0:  # Person\n",
    "            \n",
    "            grid_person[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "            \n",
    "        elif class_ids_nms[i]==1:  # Bicycle\n",
    "            \n",
    "            grid_bicycle[y:y_plus_h+1,x:x_plus_w+1] = present_val\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "    return grid_vehicles\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial and Time Scaling Coefficients\n",
    "* \"seconds\" represents the time intervals in seconds at which samples will be taken from the video.    \n",
    "* \"grid_scaling_factor\" represents the factor by which the grid will be shrunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds = 0.5\n",
    "grid_scaling_factor = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Background Areas #############\n",
    "\n",
    "#lanes\n",
    "\n",
    "lane_pts = []\n",
    "lane_pts_1 = np.array([[540,0],[483,303],[314,555],[0,915],[0,1080],[625,1080],[708,719],[745,544],[763,362],[687,154],[791,310],[822,396],[835,582],[806,861],[779,1080],[1114,1080],[1060,664],[988,418],[924,317],[662,0]], np.int32)\n",
    "lane_pts_1 = lane_pts_1.reshape((-1,1,2))\n",
    "\n",
    "lane_pts_2 = np.array([[1920,600],[1832,660],[1760,773],[1692,907],[1660,1080],[1140,1080],[1245,893],[1418,691],[1590,569],[1782,480],[1920,421]], np.int32)\n",
    "lane_pts_2 = lane_pts_2.reshape((-1,1,2))\n",
    "\n",
    "lane_pts.append(lane_pts_1)\n",
    "lane_pts.append(lane_pts_2)\n",
    "\n",
    "\n",
    "lane_pts_grid = []\n",
    "\n",
    "for pts in lane_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    lane_pts_grid.append(grid_pts)\n",
    "\n",
    "\n",
    "\n",
    "#sidewalks\n",
    "\n",
    "swalk_pts = []\n",
    "swalk_pts_1 = np.array([[454,0],[540,0],[514,143],[448,149],[383,306],[483,303],[314,555],[0,915],[0,425],[179,356]], np.int32)\n",
    "swalk_pts_1 = swalk_pts_1.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_2 = np.array([[986,265],[1173,264],[1268,319],[1484,291],[1600,296],[1920,224],[1920,281],[1280,529],[1151,500]], np.int32)\n",
    "swalk_pts_2 = swalk_pts_2.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_3 = np.array([[1920,600],[1920,1080],[1660,1080],[1692,907],[1760,773],[1832,660]], np.int32)\n",
    "swalk_pts_3 = swalk_pts_3.reshape((-1,1,2))\n",
    "\n",
    "\n",
    "swalk_pts.append(swalk_pts_1)\n",
    "swalk_pts.append(swalk_pts_2)\n",
    "swalk_pts.append(swalk_pts_3)\n",
    "\n",
    "\n",
    "swalk_pts_grid = []\n",
    "\n",
    "for pts in swalk_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    swalk_pts_grid.append(grid_pts)\n",
    "    \n",
    "\n",
    "\n",
    "#out of bounds (oob) areas\n",
    "\n",
    "\n",
    "#median spaces\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Grid Weights #############\n",
    "\n",
    "\n",
    "lane_w = 32\n",
    "swalk_w = -32\n",
    "oob_w = -127\n",
    "med_sp_w = 0\n",
    "\n",
    "\n",
    "vehicle_w = 64\n",
    "person_w = -64\n",
    "bicycle_w = 32\n",
    "\n",
    "# obj_w = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################################################### \n",
    "\n",
    "classes = None\n",
    "\n",
    "with open('yolov3.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "GREYSCALE = np.random.uniform(0, 255, size=(len(classes)))\n",
    "\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "\n",
    "#image = cv2.imread('dog.jpg')\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 32,  32,  32, ..., -32, -32, -32],\n",
       "       [ 32,  32,  32, ..., -32, -32, -32],\n",
       "       [ 32,  32,  32, ..., -32, -32, -32]], dtype=int8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "videofile = 'videos/4k_traffic_camera_video.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "success,image = cap.read()\n",
    "\n",
    "\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) # Gets the total number of frames\n",
    "\n",
    "\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # Gets the frame width\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Gets the frame width\n",
    "\n",
    "grid_x_size = int(round(frame_width/grid_scaling_factor))\n",
    "grid_y_size = int(round(frame_height/grid_scaling_factor))\n",
    "\n",
    "\n",
    "multiplier = int(round(fps * seconds))\n",
    "num_scenes = int(round(num_frames/(fps * seconds))+1)\n",
    "\n",
    "\n",
    "grids = np.zeros((num_scenes,grid_y_size,grid_x_size), dtype = bool)\n",
    "frameIds = np.zeros((num_scenes), dtype = np.int32)\n",
    "\n",
    "\n",
    "#################### Initiate Process ################\n",
    "\n",
    "#################### Create Background Grid ################\n",
    "\n",
    "grid_background = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "\n",
    "# Mark background\n",
    "cv2.fillPoly(grid_background,lane_pts_grid,lane_w)\n",
    "cv2.fillPoly(grid_background,swalk_pts_grid,swalk_w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "i = 0\n",
    "\n",
    "while success:\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        \n",
    "\n",
    "        \n",
    "#############################################################\n",
    "\n",
    "        \n",
    "        boxes_nms, class_ids_nms, confidences_nms = get_yolo_output(image)\n",
    "        \n",
    "        \n",
    "        grid = create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor,grid_background)\n",
    "        grid_rep = np.array(grid,dtype=np.uint8)\n",
    "        grid_rep = np.multiply(grid_rep,255)\n",
    "        cv2.imwrite(\"output/test/rep%d.jpg\" % frameId, grid_rep)\n",
    "        \n",
    "        grids[i] = grid\n",
    "        frameIds[i] = frameId\n",
    "        \n",
    "        \n",
    "        i= i+1\n",
    "        \n",
    "        \n",
    "        if frameId % (multiplier) == 0:\n",
    "        \n",
    "            grid_image = create_grid_image(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor,grid_background)\n",
    "            grid_image_16_bit = np.array(grid_image,dtype=np.int16)\n",
    "            grid_image_16_bit = np.add(grid_image_16_bit,128)\n",
    "            grid_image_unsigned = np.array(grid_image_16_bit,dtype=np.uint8)\n",
    "            cv2.imwrite(\"output/grid%d.jpg\" % frameId, grid_image_unsigned)\n",
    "\n",
    "            print_on_image(image, boxes_nms, class_ids_nms, confidences_nms)\n",
    "\n",
    "            cv2.imwrite(\"output/frame%d.jpg\" % frameId, image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print ('One image done')\n",
    "        \n",
    "        \n",
    "    frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "    success, image = cap.read()\n",
    "\n",
    "\n",
    "        #plt.figure(figsize = (15, 15))\n",
    "        #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    \n",
    "#cv2.imwrite(\"object-detection.jpg\", image)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "print (i)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "np.save(\"grid_arrays/grids.npy\", grids)\n",
    "np.save(\"grid_arrays/frameIds.npy\", frameIds)\n",
    "\n",
    "print (\"Complete\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = np.load(\"grid_arrays/grids.npy\")\n",
    "frameIds = np.load(\"grid_arrays/frameIds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "j = 0\n",
    "for i in grids[2].flat:\n",
    "    if i == True:\n",
    "        j = j+1\n",
    "        \n",
    "print(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithms on Grids\n",
    "## Divergence from Average Inquiry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795\n",
      "810\n",
      "840\n",
      "855\n",
      "870\n",
      "1380\n",
      "1395\n",
      "3435\n",
      "4065\n",
      "4245\n",
      "6330\n",
      "6780\n",
      "7290\n",
      "7305\n",
      "134.63032341677996\n"
     ]
    }
   ],
   "source": [
    "mean_grid = np.mean(grids, axis=0)\n",
    "\n",
    "threshold = 300\n",
    "\n",
    "dists = 0\n",
    "j = 0\n",
    "for grid,frameId  in zip(grids,frameIds):\n",
    "    grid_numeric = np.array(grid, dtype = np.float64)\n",
    "    var = distance.sqeuclidean(np.ravel(grid_numeric),np.ravel(mean_grid))\n",
    "    dists = dists + var\n",
    "    if var > threshold:\n",
    "        j = j+1\n",
    "        print(frameId)\n",
    "\n",
    "\n",
    "mean = dists/frameIds.size\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.2\n",
    "\n",
    "train_end_ind = round((1-test_size)*len(grids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 34, 60)\n",
      "613\n",
      "(613, 34, 60)\n",
      "(490, 2040)\n",
      "(123, 2040)\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "print(grids.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = grids.reshape((len(grids), np.prod(grids.shape[1:])))[:train_end_ind], grids.reshape((len(grids), np.prod(grids.shape[1:])))[train_end_ind:]\n",
    "Id_train, Id_test = frameIds[:train_end_ind], frameIds[train_end_ind:]\n",
    "\n",
    "print(len(grids))\n",
    "\n",
    "#X_train, X_test, Id_train, Id_test = train_test_split(grids.reshape((len(grids), np.prod(grids.shape[1:]))), frameIds, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "print(grids.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(34*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n"
     ]
    }
   ],
   "source": [
    "input_dim = np.prod(grids.shape[1:])\n",
    "encoding_dim = 14\n",
    "\n",
    "print(input_dim)\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 490 samples, validate on 123 samples\n",
      "Epoch 1/100\n",
      "490/490 [==============================] - 2s 4ms/step - loss: 0.1073 - acc: 0.0041 - val_loss: 0.1021 - val_acc: 0.0081\n",
      "Epoch 2/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0974 - acc: 0.0102 - val_loss: 0.0922 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0876 - acc: 0.0122 - val_loss: 0.0841 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "490/490 [==============================] - 0s 219us/step - loss: 0.0805 - acc: 0.0122 - val_loss: 0.0786 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0760 - acc: 0.0102 - val_loss: 0.0754 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "490/490 [==============================] - 0s 242us/step - loss: 0.0731 - acc: 0.0082 - val_loss: 0.0729 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "490/490 [==============================] - 0s 237us/step - loss: 0.0714 - acc: 0.0020 - val_loss: 0.0714 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "490/490 [==============================] - 0s 223us/step - loss: 0.0704 - acc: 0.0000e+00 - val_loss: 0.0713 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "490/490 [==============================] - 0s 228us/step - loss: 0.0702 - acc: 0.0000e+00 - val_loss: 0.0704 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "490/490 [==============================] - 0s 225us/step - loss: 0.0693 - acc: 0.0000e+00 - val_loss: 0.0702 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0690 - acc: 0.0000e+00 - val_loss: 0.0695 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "490/490 [==============================] - 0s 253us/step - loss: 0.0678 - acc: 0.0000e+00 - val_loss: 0.0695 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "490/490 [==============================] - 0s 249us/step - loss: 0.0680 - acc: 0.0000e+00 - val_loss: 0.0685 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "490/490 [==============================] - 0s 219us/step - loss: 0.0674 - acc: 0.0000e+00 - val_loss: 0.0690 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0678 - acc: 0.0000e+00 - val_loss: 0.0688 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "490/490 [==============================] - 0s 228us/step - loss: 0.0674 - acc: 0.0000e+00 - val_loss: 0.0685 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "490/490 [==============================] - 0s 242us/step - loss: 0.0671 - acc: 0.0000e+00 - val_loss: 0.0681 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "490/490 [==============================] - 0s 222us/step - loss: 0.0673 - acc: 0.0000e+00 - val_loss: 0.0691 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "490/490 [==============================] - 0s 222us/step - loss: 0.0667 - acc: 0.0000e+00 - val_loss: 0.0677 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "490/490 [==============================] - 0s 259us/step - loss: 0.0664 - acc: 0.0000e+00 - val_loss: 0.0677 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "490/490 [==============================] - 0s 246us/step - loss: 0.0661 - acc: 0.0000e+00 - val_loss: 0.0673 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "490/490 [==============================] - 0s 242us/step - loss: 0.0660 - acc: 0.0000e+00 - val_loss: 0.0673 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "490/490 [==============================] - 0s 257us/step - loss: 0.0656 - acc: 0.0000e+00 - val_loss: 0.0677 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "490/490 [==============================] - 0s 226us/step - loss: 0.0655 - acc: 0.0000e+00 - val_loss: 0.0671 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "490/490 [==============================] - 0s 220us/step - loss: 0.0659 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "490/490 [==============================] - 0s 226us/step - loss: 0.0661 - acc: 0.0000e+00 - val_loss: 0.0678 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0660 - acc: 0.0000e+00 - val_loss: 0.0671 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "490/490 [==============================] - 0s 255us/step - loss: 0.0652 - acc: 0.0000e+00 - val_loss: 0.0668 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0647 - acc: 0.0000e+00 - val_loss: 0.0662 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "490/490 [==============================] - 0s 254us/step - loss: 0.0643 - acc: 0.0000e+00 - val_loss: 0.0657 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0639 - acc: 0.0000e+00 - val_loss: 0.0661 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "490/490 [==============================] - 0s 231us/step - loss: 0.0636 - acc: 0.0000e+00 - val_loss: 0.0654 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "490/490 [==============================] - 0s 227us/step - loss: 0.0632 - acc: 0.0000e+00 - val_loss: 0.0654 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "490/490 [==============================] - 0s 230us/step - loss: 0.0630 - acc: 0.0000e+00 - val_loss: 0.0650 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "490/490 [==============================] - 0s 225us/step - loss: 0.0627 - acc: 0.0000e+00 - val_loss: 0.0649 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0622 - acc: 0.0000e+00 - val_loss: 0.0646 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "490/490 [==============================] - 0s 260us/step - loss: 0.0619 - acc: 0.0000e+00 - val_loss: 0.0644 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "490/490 [==============================] - 0s 239us/step - loss: 0.0620 - acc: 0.0000e+00 - val_loss: 0.0664 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0620 - acc: 0.0000e+00 - val_loss: 0.0646 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "490/490 [==============================] - 0s 257us/step - loss: 0.0620 - acc: 0.0000e+00 - val_loss: 0.0642 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "490/490 [==============================] - 0s 248us/step - loss: 0.0618 - acc: 0.0000e+00 - val_loss: 0.0636 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "490/490 [==============================] - 0s 262us/step - loss: 0.0609 - acc: 0.0000e+00 - val_loss: 0.0635 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "490/490 [==============================] - 0s 239us/step - loss: 0.0607 - acc: 0.0000e+00 - val_loss: 0.0640 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "490/490 [==============================] - 0s 241us/step - loss: 0.0604 - acc: 0.0000e+00 - val_loss: 0.0623 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "490/490 [==============================] - 0s 240us/step - loss: 0.0597 - acc: 0.0000e+00 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "490/490 [==============================] - 0s 222us/step - loss: 0.0595 - acc: 0.0000e+00 - val_loss: 0.0623 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "490/490 [==============================] - 0s 230us/step - loss: 0.0598 - acc: 0.0000e+00 - val_loss: 0.0627 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "490/490 [==============================] - 0s 238us/step - loss: 0.0601 - acc: 0.0000e+00 - val_loss: 0.0635 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "490/490 [==============================] - 0s 236us/step - loss: 0.0605 - acc: 0.0000e+00 - val_loss: 0.0627 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "490/490 [==============================] - 0s 237us/step - loss: 0.0602 - acc: 0.0000e+00 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0596 - acc: 0.0000e+00 - val_loss: 0.0622 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "490/490 [==============================] - 0s 223us/step - loss: 0.0595 - acc: 0.0000e+00 - val_loss: 0.0623 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "490/490 [==============================] - 0s 232us/step - loss: 0.0592 - acc: 0.0000e+00 - val_loss: 0.0618 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "490/490 [==============================] - 0s 236us/step - loss: 0.0590 - acc: 0.0000e+00 - val_loss: 0.0618 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "490/490 [==============================] - 0s 246us/step - loss: 0.0586 - acc: 0.0000e+00 - val_loss: 0.0616 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "490/490 [==============================] - 0s 246us/step - loss: 0.0585 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "490/490 [==============================] - 0s 244us/step - loss: 0.0584 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "490/490 [==============================] - 0s 235us/step - loss: 0.0588 - acc: 0.0000e+00 - val_loss: 0.0620 - val_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0591 - acc: 0.0000e+00 - val_loss: 0.0619 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0592 - acc: 0.0000e+00 - val_loss: 0.0631 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "490/490 [==============================] - 0s 230us/step - loss: 0.0591 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "490/490 [==============================] - 0s 224us/step - loss: 0.0586 - acc: 0.0000e+00 - val_loss: 0.0615 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "490/490 [==============================] - 0s 229us/step - loss: 0.0584 - acc: 0.0000e+00 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0582 - acc: 0.0000e+00 - val_loss: 0.0610 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "490/490 [==============================] - 0s 239us/step - loss: 0.0585 - acc: 0.0000e+00 - val_loss: 0.0616 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0581 - acc: 0.0000e+00 - val_loss: 0.0612 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "490/490 [==============================] - 0s 245us/step - loss: 0.0580 - acc: 0.0000e+00 - val_loss: 0.0611 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "490/490 [==============================] - 0s 244us/step - loss: 0.0581 - acc: 0.0000e+00 - val_loss: 0.0610 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "490/490 [==============================] - 0s 240us/step - loss: 0.0580 - acc: 0.0000e+00 - val_loss: 0.0607 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "490/490 [==============================] - 0s 225us/step - loss: 0.0576 - acc: 0.0000e+00 - val_loss: 0.0607 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "490/490 [==============================] - 0s 240us/step - loss: 0.0574 - acc: 0.0000e+00 - val_loss: 0.0609 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "490/490 [==============================] - 0s 249us/step - loss: 0.0571 - acc: 0.0000e+00 - val_loss: 0.0603 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "490/490 [==============================] - 0s 235us/step - loss: 0.0569 - acc: 0.0000e+00 - val_loss: 0.0608 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "490/490 [==============================] - 0s 237us/step - loss: 0.0571 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "490/490 [==============================] - 0s 239us/step - loss: 0.0572 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "490/490 [==============================] - 0s 243us/step - loss: 0.0569 - acc: 0.0000e+00 - val_loss: 0.0606 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "490/490 [==============================] - 0s 231us/step - loss: 0.0568 - acc: 0.0000e+00 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "490/490 [==============================] - 0s 235us/step - loss: 0.0566 - acc: 0.0000e+00 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "490/490 [==============================] - 0s 236us/step - loss: 0.0566 - acc: 0.0000e+00 - val_loss: 0.0600 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "490/490 [==============================] - 0s 250us/step - loss: 0.0565 - acc: 0.0000e+00 - val_loss: 0.0601 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "490/490 [==============================] - 0s 246us/step - loss: 0.0566 - acc: 0.0000e+00 - val_loss: 0.0600 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "490/490 [==============================] - 0s 236us/step - loss: 0.0567 - acc: 0.0000e+00 - val_loss: 0.0602 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "490/490 [==============================] - 0s 233us/step - loss: 0.0568 - acc: 0.0000e+00 - val_loss: 0.0611 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "490/490 [==============================] - 0s 242us/step - loss: 0.0571 - acc: 0.0000e+00 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "490/490 [==============================] - 0s 238us/step - loss: 0.0568 - acc: 0.0000e+00 - val_loss: 0.0598 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "490/490 [==============================] - 0s 228us/step - loss: 0.0568 - acc: 0.0000e+00 - val_loss: 0.0608 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "490/490 [==============================] - 0s 226us/step - loss: 0.0565 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "490/490 [==============================] - 0s 235us/step - loss: 0.0563 - acc: 0.0000e+00 - val_loss: 0.0593 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "490/490 [==============================] - 0s 240us/step - loss: 0.0562 - acc: 0.0000e+00 - val_loss: 0.0608 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "490/490 [==============================] - 0s 238us/step - loss: 0.0562 - acc: 0.0000e+00 - val_loss: 0.0597 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "490/490 [==============================] - 0s 227us/step - loss: 0.0557 - acc: 0.0000e+00 - val_loss: 0.0596 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "490/490 [==============================] - 0s 231us/step - loss: 0.0556 - acc: 0.0000e+00 - val_loss: 0.0595 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "490/490 [==============================] - 0s 227us/step - loss: 0.0559 - acc: 0.0000e+00 - val_loss: 0.0606 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "490/490 [==============================] - 0s 270us/step - loss: 0.0560 - acc: 0.0000e+00 - val_loss: 0.0596 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "490/490 [==============================] - 0s 262us/step - loss: 0.0561 - acc: 0.0000e+00 - val_loss: 0.0593 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "490/490 [==============================] - 0s 230us/step - loss: 0.0560 - acc: 0.0000e+00 - val_loss: 0.0593 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "490/490 [==============================] - 0s 237us/step - loss: 0.0555 - acc: 0.0000e+00 - val_loss: 0.0594 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "490/490 [==============================] - 0s 225us/step - loss: 0.0554 - acc: 0.0000e+00 - val_loss: 0.0588 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "490/490 [==============================] - 0s 228us/step - loss: 0.0549 - acc: 0.0000e+00 - val_loss: 0.0594 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "490/490 [==============================] - 0s 260us/step - loss: 0.0548 - acc: 0.0000e+00 - val_loss: 0.0590 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"models/model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'frameId': Id_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7350, 7365, 7380, 7425, 7440, 7515, 7575, 7590, 7605, 7620, 7635, 7650, 7665, 7890, 7905, 7920, 7935, 7950, 8190, 8250, 8385, 8400, 8475, 8505, 8550, 8565, 8655, 8670, 8775, 8790, 8805, 8820, 8880, 8910, 9030, 9045, 9060, 9075, 9165, 9180]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.05\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "pred_frameIds = []\n",
    "for y,frameId in zip(y_pred, Id_test):\n",
    "    if y==1:\n",
    "        pred_frameIds.append(frameId)\n",
    "pred_frameIds.sort()\n",
    "print(pred_frameIds)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 34, 60)\n",
      "613\n",
      "(613, 34, 60)\n",
      "(490, 34, 60, 1)\n",
      "(123, 34, 60, 1)\n",
      "2040\n"
     ]
    }
   ],
   "source": [
    "print(grids.shape)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = grids.reshape((len(grids), grids.shape[1], grids.shape[2], 1))[:train_end_ind], grids.reshape((len(grids), grids.shape[1], grids.shape[2], 1))[train_end_ind:]\n",
    "Id_train, Id_test = frameIds[:train_end_ind], frameIds[train_end_ind:]\n",
    "\n",
    "print(len(grids))\n",
    "\n",
    "#X_train, X_test, Id_train, Id_test = train_test_split(grids.reshape((len(grids), np.prod(grids.shape[1:]))), frameIds, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "print(grids.shape)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(34*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "input_dim = (grids.shape[1], grids.shape[2], 1)\n",
    "\n",
    "print(input_dim)\n",
    "\n",
    "\n",
    "input_img = Input(shape=input_dim)  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (4, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 490 samples, validate on 123 samples\n",
      "Epoch 1/50\n",
      "490/490 [==============================] - 6s 12ms/step - loss: 0.6002 - val_loss: 0.4405\n",
      "Epoch 2/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.4441 - val_loss: 0.2824\n",
      "Epoch 3/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.2763 - val_loss: 0.1542\n",
      "Epoch 4/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1811 - val_loss: 0.1251\n",
      "Epoch 5/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1575 - val_loss: 0.1141\n",
      "Epoch 6/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1441 - val_loss: 0.1072\n",
      "Epoch 7/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1345 - val_loss: 0.1011\n",
      "Epoch 8/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1277 - val_loss: 0.0998\n",
      "Epoch 9/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1228 - val_loss: 0.0939\n",
      "Epoch 10/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1181 - val_loss: 0.0911\n",
      "Epoch 11/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1143 - val_loss: 0.0891\n",
      "Epoch 12/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1112 - val_loss: 0.0861\n",
      "Epoch 13/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1078 - val_loss: 0.0856\n",
      "Epoch 14/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1053 - val_loss: 0.0820\n",
      "Epoch 15/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.1021 - val_loss: 0.0801\n",
      "Epoch 16/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0994 - val_loss: 0.0787\n",
      "Epoch 17/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0971 - val_loss: 0.0766\n",
      "Epoch 18/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0945 - val_loss: 0.0751\n",
      "Epoch 19/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0924 - val_loss: 0.0740\n",
      "Epoch 20/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0908 - val_loss: 0.0718\n",
      "Epoch 21/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0889 - val_loss: 0.0709\n",
      "Epoch 22/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0877 - val_loss: 0.0708\n",
      "Epoch 23/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0861 - val_loss: 0.0686\n",
      "Epoch 24/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0849 - val_loss: 0.0686\n",
      "Epoch 25/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0836 - val_loss: 0.0676\n",
      "Epoch 26/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0829 - val_loss: 0.0675\n",
      "Epoch 27/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0829 - val_loss: 0.0674\n",
      "Epoch 28/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0809 - val_loss: 0.0651\n",
      "Epoch 29/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0798 - val_loss: 0.0646\n",
      "Epoch 30/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0788 - val_loss: 0.0640\n",
      "Epoch 31/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0786 - val_loss: 0.0656\n",
      "Epoch 32/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0786 - val_loss: 0.0643\n",
      "Epoch 33/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0768 - val_loss: 0.0632\n",
      "Epoch 34/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0761 - val_loss: 0.0621\n",
      "Epoch 35/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0753 - val_loss: 0.0626\n",
      "Epoch 36/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0751 - val_loss: 0.0614\n",
      "Epoch 37/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0745 - val_loss: 0.0610\n",
      "Epoch 38/50\n",
      "490/490 [==============================] - 4s 7ms/step - loss: 0.0736 - val_loss: 0.0606\n",
      "Epoch 39/50\n",
      "490/490 [==============================] - 4s 8ms/step - loss: 0.0732 - val_loss: 0.0613\n",
      "Epoch 40/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0735 - val_loss: 0.0605\n",
      "Epoch 41/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0724 - val_loss: 0.0611\n",
      "Epoch 42/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0717 - val_loss: 0.0592\n",
      "Epoch 43/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0717 - val_loss: 0.0594\n",
      "Epoch 44/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0722 - val_loss: 0.0602\n",
      "Epoch 45/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0708 - val_loss: 0.0584\n",
      "Epoch 46/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0699 - val_loss: 0.0588\n",
      "Epoch 47/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0697 - val_loss: 0.0578\n",
      "Epoch 48/50\n",
      "490/490 [==============================] - 3s 7ms/step - loss: 0.0691 - val_loss: 0.0579\n",
      "Epoch 49/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0690 - val_loss: 0.0577\n",
      "Epoch 50/50\n",
      "490/490 [==============================] - 3s 6ms/step - loss: 0.0687 - val_loss: 0.0570\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"models/model_2D_conv.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                epochs=50,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test),\n",
    "                callbacks=[checkpointer]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = load_model('models/model_2D_conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        (None, 34, 60, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 34, 60, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 17, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 17, 30, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 9, 15, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 9, 15, 8)          584       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 5, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 5, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 10, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 10, 16, 8)         584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 20, 32, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 17, 30, 16)        1552      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 34, 60, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 34, 60, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,769\n",
      "Trainable params: 4,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3X2cnHV97//XZ+529m42m+xudnNHAgYJAUwgRKitBSvIjYKtCkjhWNuf2POoP9tfRYWeSo+c9tTWc6zWUhUrR89RoQhqowZBFASPAgmICCQhAYLZ3O1mk72/3/n8/riumZ3dzCabZGd3s9f7+Xhcj7nmmmuu+V5hmfd8v9/r+73M3REREQGIzXQBRERk9lAoiIhInkJBRETyFAoiIpKnUBARkTyFgoiI5CkURCbJzL5qZn87yX13mtlbT/Q4ItNNoSAiInkKBRERyVMoyJwSNtt81MyeM7MeM/uKmS00swfMrMvMHjaz2oL9rzKzF8ys3cweNbNVBa+tNbNnwvf9O5Ae91lvN7Nnw/f+3MzOOc4yf8DMdpjZQTPbYGaLwu1mZv9kZi1m1hGe01nha1eY2Yth2Xab2c3H9Q8mMo5CQeaidwGXAKcD7wAeAP4KqCP4m/8wgJmdDtwN/AVQD2wEvmdmKTNLAd8F/g8wH/hWeFzC954L3AV8EFgAfAnYYGZlx1JQM3sL8PfANUAT8BpwT/jypcCbw/OYB1wLtIWvfQX4oLtXA2cBPzmWzxWZiEJB5qLPu/t+d98NPA486e6/dPcB4DvA2nC/a4EfuPuP3H0I+B9AOfBbwAVAEvisuw+5+33ApoLP+ADwJXd/0t1H3P1rwED4vmPxh8Bd7v5MWL5bgQvNbDkwBFQDZwDm7lvcfW/4viHgTDPLuPshd3/mGD9XpCiFgsxF+wvW+4o8rwrXFxH8MgfA3bPALmBx+NpuHztj5GsF66cAHwmbjtrNrB1YGr7vWIwvQzdBbWCxu/8E+BfgDmC/md1pZplw13cBVwCvmdlPzezCY/xckaIUChJlewi+3IGgDZ/gi303sBdYHG7LWVawvgv4O3efV7BUuPvdJ1iGSoLmqN0A7v7P7n4esJqgGemj4fZN7n410EDQzHXvMX6uSFEKBYmye4Erzez3zCwJfISgCejnwC+AYeDDZpYwsz8A1he898vAn5rZG8MO4Uozu9LMqo+xDN8E3m9ma8L+iP9O0Ny108zOD4+fBHqAfmAk7PP4QzOrCZu9OoGRE/h3EMlTKEhkufs24Abg88ABgk7pd7j7oLsPAn8A/BFwiKD/4dsF791M0K/wL+HrO8J9j7UMPwY+AdxPUDs5DbgufDlDED6HCJqY2gj6PQBuBHaaWSfwp+F5iJww0012REQkRzUFERHJUyiIiEieQkFERPIUCiIikpeY6QIcq7q6Ol++fPlMF0NE5KTy9NNPH3D3+qPtd9KFwvLly9m8efNMF0NE5KRiZq8dfS81H4mISAGFgoiI5CkUREQk76TrUxAROR5DQ0M0NzfT398/00UpqXQ6zZIlS0gmk8f1foWCiERCc3Mz1dXVLF++nLGT384d7k5bWxvNzc2sWLHiuI5R0uYjM7vMzLaFtxq8ZYJ9rglvK/iCmX2zlOURkejq7+9nwYIFczYQAMyMBQsWnFBtqGQ1BTOLE9wc5BKgGdhkZhvc/cWCfVYS3GnqTe5+yMwaSlUeEZG5HAg5J3qOpawprAd2uPsr4TTE9wBXj9vnA8Ad7n4IwN1bSlWYTTsP8g8/3IpmhRURmVgpQ2Exwd2pcprDbYVOB043s/9rZk+Y2WXFDmRmN5nZZjPb3NraelyFea65gy88+jLtvUPH9X4RkRPR3t7Ov/7rvx7z+6644gra29tLUKLiShkKxeow43+mJ4CVwEXAe4F/M7N5h73J/U53X+fu6+rrjzpKu6immjQAezvm9pUHIjI7TRQKIyNHvmnexo0bmTfvsK/FkillKDQT3O82ZwnB/WjH7/Mf7j7k7q8C2whCYso1hqGwr7OvFIcXETmiW265hZdffpk1a9Zw/vnnc/HFF3P99ddz9tlnA/DOd76T8847j9WrV3PnnXfm37d8+XIOHDjAzp07WbVqFR/4wAdYvXo1l156KX19U/99VspLUjcBK81sBcFNyK8Drh+3z3cJaghfNbM6guakV0pRGNUURCTnk997gRf3dE7pMc9clOFv3rF6wtc/9alP8fzzz/Pss8/y6KOPcuWVV/L888/nLx296667mD9/Pn19fZx//vm8613vYsGCBWOOsX37du6++26+/OUvc80113D//fdzww1TeyfWktUU3H0Y+BDwILAFuNfdXzCz283sqnC3B4E2M3sReAT4qLu3laI89VVlxAz2KRREZBZYv379mLEE//zP/8wb3vAGLrjgAnbt2sX27dsPe8+KFStYs2YNAOeddx47d+6c8nKVdPCau28ENo7bdlvBugN/GS4llYjHaKhOq6YgIkf8RT9dKisr8+uPPvooDz/8ML/4xS+oqKjgoosuKjrWoKysLL8ej8dL0nwUqbmPGmvSqimIyIyorq6mq6ur6GsdHR3U1tZSUVHB1q1beeKJJ6a5dKMiNc1FU02al/YX/48iIlJKCxYs4E1vehNnnXUW5eXlLFy4MP/aZZddxhe/+EXOOeccXv/613PBBRfMWDkjFQqNNWl++lIr7h6JkY0iMrt885vFZ/IpKyvjgQceKPpart+grq6O559/Pr/95ptvnvLyQcSaj5pq0vQOjtA1MDzTRRERmZUiFQqNNeWArkASEZlIpEJBYxVERI4sUqHQmAlHNXdoVLOISDGRCoWFGdUURESOJFKhkErEqKsqU5+CiMgEIhUKEPQrqKYgItPteKfOBvjsZz9Lb2/vFJeouMiFgkY1i8hMOFlCIVKD1yCoKTz5Sknm3BMRmVDh1NmXXHIJDQ0N3HvvvQwMDPD7v//7fPKTn6Snp4drrrmG5uZmRkZG+MQnPsH+/fvZs2cPF198MXV1dTzyyCMlLWfkQqGxJk1n/zA9A8NUlkXu9EUE4IFbYN+vp/aYjWfD5Z+a8OXCqbMfeugh7rvvPp566incnauuuorHHnuM1tZWFi1axA9+8AMgmBOppqaGz3zmMzzyyCPU1dVNbZmLiFzzUVP+ZjtqQhKRmfHQQw/x0EMPsXbtWs4991y2bt3K9u3bOfvss3n44Yf5+Mc/zuOPP05NTc20ly1yP5UbM6Ojmk+rr5rh0ojIjDjCL/rp4O7ceuutfPCDHzzstaeffpqNGzdy6623cumll3LbbbcVOULpRLamoCuQRGQ6FU6d/ba3vY277rqL7u5uAHbv3k1LSwt79uyhoqKCG264gZtvvplnnnnmsPeWWvRqCjUa1Swi069w6uzLL7+c66+/ngsvvBCAqqoqvv71r7Njxw4++tGPEovFSCaTfOELXwDgpptu4vLLL6epqankHc0W3Pzs5LFu3TrfvHnzCR1j7e0PceU5TfztO8+eolKJyGy3ZcsWVq1aNdPFmBbFztXMnnb3dUd7b+SajyCYLVVjFUREDhfJUNCoZhGR4iIZChrVLBJNJ1tz+fE40XOMZCg0ZdK09QzSPzQy00URkWmSTqdpa2ub08Hg7rS1tZFOp4/7GJG7+ghGr0Bq6Rxg2YKKGS6NiEyHJUuW0NzcTGtr60wXpaTS6TRLliw57vdHMhSawtty7u3oUyiIREQymWTFihUzXYxZL5LNR42a6kJEpKhIh4KuQBIRGSuSoVBVlqA6ndAVSCIi40QyFCA3VkFTXYiIFCppKJjZZWa2zcx2mNktRV7/IzNrNbNnw+X/KWV5CmlUs4jI4Up29ZGZxYE7gEuAZmCTmW1w9xfH7frv7v6hUpVjIk2ZNFv3dk73x4qIzGqlrCmsB3a4+yvuPgjcA1xdws87Jo01aVq7Bxgayc50UUREZo1ShsJiYFfB8+Zw23jvMrPnzOw+M1ta7EBmdpOZbTazzVM18KSpJo07tHQNTMnxRETmglKGghXZNn58+feA5e5+DvAw8LViB3L3O919nbuvq6+vn5LC6b4KIiKHK2UoNAOFv/yXAHsKd3D3NnfP/VT/MnBeCcszxuioZnU2i4jklDIUNgErzWyFmaWA64ANhTuYWVPB06uALSUszxijNQWFgohITsmuPnL3YTP7EPAgEAfucvcXzOx2YLO7bwA+bGZXAcPAQeCPSlWe8TLpBBWpuGoKIiIFSjohnrtvBDaO23ZbwfqtwK2lLMNEzEz3VRARGSeyI5pBo5pFRMaLdCg0ZjSqWUSkUKRDoakmzf6uAUayc/dOTCIixyLSodBYk2Yk6xzo1gA2ERGIeCg06b4KIiJjRDoUNKpZRGSsSIeCRjWLiIwV6VCorUiSSsR0BZKISCjSoWBm4VgFhYKICEQ8FAAaMxrVLCKSE/lQaKpJs7dTHc0iIqBQoLGmnP0dA2Q1gE1ERKHQVJNmcCTLwd7BmS6KiMiMi3wo5MYq7G1Xv4KISHRCYd+v4ckvgY9tJmrMBKGwv1OhICISnVB45afwwMegv2PM5oW5UOhSKIiIRCcUMouCx669YzbXVaUwg/2dmhRPRCR6odC5e8zmRDxGXVUZLWo+EhGJYijsOeylhZky9SmIiBClUKhqBAw69x720sLqtJqPRESIUigkUlBZf1jzEUBDJk2LOppFRCIUChA0IU3QfHSge5ChkewMFEpEZPaIXih0FWk+Ci9Lbe1SE5KIRFv0QqFI89HCTBmgAWwiItELhb5DMDR2VtSG6tyoZtUURCTaohUK1cUvS12oqS5ERICohcIEYxUWVKaIx0yhICKRV9JQMLPLzGybme0ws1uOsN+7zczNbF0py0NmcfA4rrM5FjMaqsvUfCQikVeyUDCzOHAHcDlwJvBeMzuzyH7VwIeBJ0tVlrxMU/CosQoiIkWVsqawHtjh7q+4+yBwD3B1kf3+G/CPQOm/kVOVkK4pPlahWlNdiIiUMhQWA7sKnjeH2/LMbC2w1N2/f6QDmdlNZrbZzDa3traeWKmqJxrApqkuRERKGQpWZFv+DjdmFgP+CfjI0Q7k7ne6+zp3X1dfX39ipTrCqOaOviH6h0ZO7PgiIiexUoZCM7C04PkSoPDbuBo4C3jUzHYCFwAbSt/ZPHFNAaBFtQURibBShsImYKWZrTCzFHAdsCH3ort3uHuduy939+XAE8BV7r65hGUKQqF7P4wMjdmsO7CJiJQwFNx9GPgQ8CCwBbjX3V8ws9vN7KpSfe5RZRYBHgRDAQ1gExGBRCkP7u4bgY3jtt02wb4XlbIsebmxCp17oGZJfvPo/EdqPhKR6IrWiGaA6txYhbH9CjXlSVKJmG7LKSKRFr1QmGCqCzPTbTlFJPKiFwrltZAoLz6Ftm7LKSIRF71QMAumu5jgZju6+khEoix6oQBBZ3ORsQoNmTKNUxCRSItmKFQ3TTiArXtgmO6B4RkolIjIzItmKOTu1ZzNjtmcuyxVVyCJSFRFNBQWw8gg9LaN2bxQt+UUkYiLaCiEYxW6xjYhNeTmP1Jns4hEVERDYaJ7NedGNSsURCSaIhoKuakuxo5VqCpLUJGKq/lIRCIrmqFQWQ8Wh86xYxWCUc1p9qmmICIRFc1QiMUnvCy1obpMVx+JSGRFMxQg6GwuNtWFbsspIhE2qVAwsz83s4wFvmJmz5jZpaUuXEnlxiqMk5sUz92LvElEZG6bbE3hj929E7gUqAfeD3yqZKWaDpnF0LEbxn35L8ykGRjO0tmnUc0iEj2TDQULH68A/pe7/6pg28mpugmGemCgc8zmBt2WU0QibLKh8LSZPUQQCg+aWTWQPcp7Zrf8WIWxTUgLqzVWQUSia7K34/wTYA3wirv3mtl8giakk1fhWIWGM/KbR+/VrM5mEYmeydYULgS2uXu7md0A/DXQUbpiTYP8VBdjawoNGtUsIhE22VD4AtBrZm8APga8BvzvkpVqOkxwr+aKVILqdEJjFUQkkiYbCsMeXKN5NfA5d/8cUF26Yk2DRFkwslljFURE8ibbp9BlZrcCNwK/Y2ZxIFm6Yk2T6qbDOpoBGnVbThGJqMnWFK4FBgjGK+wDFgOfLlmppotuyykiMsakQiEMgm8ANWb2dqDf3U/uPgU44lQXLV39ZLMa1Swi0TLZaS6uAZ4C3gNcAzxpZu8uZcGmRWYR9B2EobFNRQuryxgacQ71Ds5QwUREZsZk+xT+C3C+u7cAmFk98DBwX6kKNi1yYxW69sD8U/ObC8cqLKgqm4mSiYjMiMn2KcRygRBqm8x7zewyM9tmZjvM7JYir/+pmf3azJ41s5+Z2ZmTLM/UmOCyVE11ISJRNdmawg/N7EHg7vD5tcDGI70hvELpDuASoBnYZGYb3P3Fgt2+6e5fDPe/CvgMcNkxlP/E5Ec1j5vqIhzAprEKIhI1kwoFd/+omb0LeBPBRHh3uvt3jvK29cAOd38FwMzuIRjnkA+FcObVnEpgent2c6Oax3U21+fnP9IVSCISLZOtKeDu9wP3H8OxFwO7Cp43A28cv5OZ/Rnwl0AKeEuxA5nZTcBNAMuWLTuGIhxFWTWUZQ5rPipLxJlfmdJUFyISOUfsFzCzLjPrLLJ0mVnnkd5L8am1D6sJuPsd7n4a8HGCOZUOf5P7ne6+zt3X1dfXH+Vjj1FmUdDRPE5DdZlqCiISOUesKbj7iUxl0QwsLXi+BDj823fUPQRzLE2vzKKiA9hyYxVERKKklPdo3gSsNLMVZpYCrgM2FO5gZisLnl4JbC9heYqrXlR0qovcbTlFRKJk0n0Kx8rdh83sQ8CDQBy4y91fMLPbgc3uvgH4kJm9FRgCDgHvK1V5JpRZBN37YGQY4qP/HAszaVq7BhgeyZKIlzI7RURmj5KFAoC7b2TcpavuflvB+p+X8vMnJdMEnoXu/VCzOL+5IZMm69DWM5gfzCYiMtfpJ3B+rMLYfgXdllNEokihUBP2hXf8Zsxm3ZZTRKJIoVB7SvB4aOeYzaOhoJqCiESHQiFVCZUNcPDVMZvrqlKkEjFea+uZoYKJiEw/hQJA7fLDagqJeIyVDVVs3dc1I0USEZkJCgUIQ+G1wzaf0ZhRKIhIpCgUIAiFzmYYHntTnVVN1bR2DXCgW53NIhINCgWA+SuCsQodu8ZsPqMxA8A21RZEJCIUChDUFOCwfoUzmoKpn7bsPdrcfyIic4NCASYMhbqqMuqqylRTEJHIUCgAVDVCvOywUICgX0GdzSISFQoFgFgsGMRWJBTOaKzmpf1dDI9kp79cIiLTTKGQU2SsAgSdzQPDWXa29U57kUREpptCIScXCj725nC5zuat+9TZLCJzn0Ihp3Y5DHRC36Exm1/XUEU8Zmzdq34FEZn7FAo5E1yBVJaIc2pdpWoKIhIJCoWcfCi8ethLZzRl2KKagohEgEIhZ17xKbQhuAJpd3sfnf1D01smEZFpplDIKauCyvoJxyqAprsQkblPoVDoCJelAhrEJiJznkKh0ASh0FSTJpNOsFVzIInIHKdQKFS7HDqaYWRs34GZcUaT7q0gInOfQqFQbfEptAFWNVazbV8X2awXeaOIyNygUCg0wVgFCC5L7R4YZnd737QWSURkOikUCh0pFBp1bwURmfsUCoWqmyCeKhoKpy+sxkxXIInI3KZQKBSLBYPYioRCZVmCZfMrNN2FiMxpCoXxJrgsFYImJE2MJyJzWUlDwcwuM7NtZrbDzG4p8vpfmtmLZvacmf3YzE4pZXkmpXY5HNx52BTaEAxie7Wth77BkWkvlojIdChZKJhZHLgDuBw4E3ivmZ05brdfAuvc/RzgPuAfS1WeSatdDgMdh02hDcF0F+6wvUW1BRGZm0pZU1gP7HD3V9x9ELgHuLpwB3d/xN1ztzR7AlhSwvJMzhGvQAqnu1ATkojMUaUMhcVA4Siw5nDbRP4EeKDYC2Z2k5ltNrPNra2tU1jEIo4QCsvmV1CejLNFnc0iMkeVMhSsyLaiw4HN7AZgHfDpYq+7+53uvs7d19XX109hEYuonXgK7VjMeL06m0VkDitlKDQDSwueLwH2jN/JzN4K/BfgKncfKGF5JqesGirqJrwCaVVTNVv3deJFOqJFRE52pQyFTcBKM1thZingOmBD4Q5mthb4EkEgtJSwLMfmiJelZjjUO0RL18znl4jIVCtZKLj7MPAh4EFgC3Cvu79gZreb2VXhbp8GqoBvmdmzZrZhgsNNr6OMVQBNdyEic1OilAd3943AxnHbbitYf2spP/+41S6HF74TTKEdT455qfCGOxe9vmEGCiciUjoa0VxM7XLwkeDeCuPUVCRZVJPm5y+3qV9BROYchUIx81cEjxM0Id1w4Sk89lIr33r68NAQETmZKRSKOcJYBYAPvvk0Ljh1Pv91wwu83No9bcUSESk1hUIx+Sm0Xy36cjxmfPbataQSMT589y8ZGNZcSCIyNygUionFYd6yCWsKAI01aT797jfwwp5O/vGH26avbCIiJaRQmMgRLkvNueTMhbzvwlP4ys9e5ZFts2eYhYjI8VIoTGQSoQBw6xWrOKOxmpvv/RUtXf0lL5aISCkpFCZSuxz6i0+hXSidjPP5966lZ3CYj9z7K7JZXaYqIicvhcJEjnIFUqGVC6u57e2reXz7Ab742MslLZaISCkpFCZyDKEA8N71S7ny7Cb+8Yfb+O8btzA8ki1Z0URESkWhMJHa5WAxeO3nk9rdzPina9fwvgtP4c7HXuE/3fUUbd2aNE9ETi4KhYmUVcPaG2HzXdA6uUtOU4kYn7z6LP7He97A068d4h2f/xnPNbeXuKAiIlNHoXAkv3cbJCvhgY/BMcxz9O7zlnD/f/4tzIx3f/EX3Ltp19HfJCIyCygUjqSyDi7+K3jlUdj6g2N661mLa/j+//vbvHHFfD52/3Pc+u3n6OwfKk05RUSmiELhaM7/E6hfBQ/eCkN9x/TW2soUX33/ev7zRadx91O7uPjTj/L1J15TJ7SIzFoKhaOJJ+Hyf4D238DP/+XY3x4zPn7ZGXzvQ7/N6xqq+OvvPs/ln3tcI6BFZFZSKEzGqb8LZ14Nj//PovdYmIyzl9Rwz00X8KUbz2NoJMv7/9cmbvzKk2zdpzu4icjsoVCYrEv/FnB46K+P+xBmxttWN/LQ//e7fOLtZ/JccwdXfO5x/virm/jRi/vVrCQiM85OtruHrVu3zjdv3jwzH/7op+DRv4f3fR9W/M4JH669d5B/e/xV7t28i5auARZmynjPeUu59vylLJ1fMQUFFhEJmNnT7r7uqPspFI7BUB/8y/pgDMMHH4P41Nziengky0+2tnDPpl08uq0FB377dXW845xF/O7r61mYSU/J54hIdCkUSuXFDXDvjXDRX8HvfgzMpvTwe9r7+NbmZu7dvIvd7cHVTmc2Zbj4jHouen0Da5fOIxFXq5+IHBuFQqm4B6Gw5Xuw4s3wjs/B/FNL8DHOtv1dPLK1lUe2tfD0a4cYyTqZdILfWVnPm0+v482n19NUUz7lny0ic49CoZSyWXjmq/Cjv4GRIbj4Vrjgz6asOamYjr4h/u+OAzy6rYWfvtTK/s5gXqWVDVW8+fR63nx6PW9cMZ90Ml6yMojIyUuhMB0698APboZtP4CmNXDV56HpnJJ/rLvz0v5uHnuplce2t/LkqwcZHM6SjBurF9Vw7rJazjullnNPmaeahIgACoXp4w4v/gds/Cj0tsEbPwjrb4L5K6atCH2DIzz5ahu/eKWNX77Wzq+a2xkYDi5vbapJc+6yWs5clOGMxmpWNWVoqkljU9wXIiKzm0JhuvUehB99Ap79JngWTnsLrPtjOP3ykjYrFTM4nGXL3k6efu0Qz/zmEM/uaqf50OgUHZl0gjOaMqxqrGb14hrOWlTDyoVVJNWBLTJnKRRmSsdu+OX/gae/Bl17oLoJzv1PwVKzZMaK1dk/xEv7utiyr4utezvZGj72DI4AkIrHOKOpmtWLajhrcYbXL6zm1PoqaiuSqlWIzAGzIhTM7DLgc0Ac+Dd3/9S4198MfBY4B7jO3e872jFnfSjkjAzD9oeC+zHseDjYtuLNcM61cOZVwViHGZbNOjvbenh+TyfP7+7IL539w/l9asqTnFpfyYq6Sk6rr+K0+irObMqwpLacWExhIXKymPFQMLM48BJwCdAMbALe6+4vFuyzHMgANwMb5lQoFDr0Gjz7DXjuXjj0KiTK4Ywrg4A47eJg0r1Zwt1pPtTHjpZuXm7t5tUDPbzS2sOrB3rY19mf36+qLJHvowiWal7XUEV1evaci4iMmg2hcCHwX939beHzWwHc/e+L7PtV4PtzNhRy3KF5E/zqHnjh29B3CMprg1t/ltdC+fzgsWJ+sL50PSxaO+UD5I5Xz8Aw21u62bK3ky17O3lxT9AM1T0wWrNYmCljZUMQEKc1VPG6+iqWLaigMZMmrpqFyIyZbCiUsgd0MVB4y7Fm4I3HcyAzuwm4CWDZsmUnXrKZYhZ80S9dD5d9KmhW2voD6N4PfQfh0M6gw7q/AwjDev6pcNa7gqVh1UyWnsqyBGuWzmPN0nn5bdlsULPYuq+THa3dQQ2jpZtvbd6V768ASMaNxfPKWTq/gqXzK1g2v4LlCyo5rb6SZQsqKEtofIXIbFDKUCj2s/C4qiXufidwJwQ1hRMp1KyRSMEZVwTLeNkR6DkQ9Ek8f18wZfdjn4aG1XDWH8DKS6DudEjO/BiEWMxYtqCCZQsquLRgu7uzt6Ofl1u72XWwj12HevnNwV6aD/bywK/3cqh39C50MYMltRWcWl/JqXVVLK+rYPG8cpbUVrCktpzKsum9ekskykr5f1szsLTg+RJgTwk/b+6IxaF6IZx7Y7B0t8AL34Xn74ef/LdgsVjQ7FS/ChrOgIYzoW4l1CwNmqBmuMnJzFg0r5xF84oHV2f/EDvD/opXWrt5OVx/4pU2+ofGTiFeW5FkSW0FTTVpGjJl1FcFjw3VZdRXl7Ewk6auqkzNUyJToJShsAlYaWYrgN3AdcD1Jfy8uauqAd54U7C07wr6JVq3QsuW4PGlH4KPNtWQKIeaxcElsJklMG9p0PTUsDoYVBeb+aaaTDrJOUvmcc6SeWO2Z7N77RHPAAAOZ0lEQVTOgZ4Bmg/1sftQH82H+mg+1Mvu9j5ea+tl086DY2oZOfGY0VBdRmNNmqaaNI2ZchpryqitSDGvIsW8iiS1FUlqylPUlCdJJTQmQ6SYUl+SegXBJadx4C53/zszux3Y7O4bzOx84DtALdAP7HP31Uc65knd0VwqwwPQtgPaXobO3cHd4XJL527o2ke+5S5RHtYsVgdBka6BRDpozkqkIR4+1i6HTNNMntWEBoezHOgeoKVrgJbOfvZ3DbC/o5+9Hf3s7+xnb0cfezv66S3o0xgvk05QV11GfVVZ/rG+uoz5lal8eARBEjxqTik52c341UelolA4DkN9QY1i/wuw/0VoCR97jnKf6OomWHQuLF4bPC5aG1wZdRJwd7oHhmnvHQqWvkEO9Q7R0Rs8tnUP0No9wIGuwfBxgK6Cq6jGSydj1FakgqUymV+fV5GkIpWgsixOeTJORSpBRbgeC5vwxv8/VluZoqkmrct3ZVrNhquPZLZIlgdf6IvWjt3eexAGu2F4EEYGYLg/WB/qhQPbYc8zsPvpYMK/nOomyCwOmqcyiyGzaPSxog4qF0B63qzo06hOJ6lOJ1k6yRzrGxzhUO9gGCSDtPcFgRJsC8KkvXeQgz2D7G3v5GDvIB19Qxzv76rqdIJFNeU0zUvTVFNOfXUZNeVJMukEmfJkuJ4kU56gOp2kqiyhfhMpOYVClFXMn/iX/+t+b3S9rx32Pgu7nwmbqJqD/oztD8NQz+HvjSWgYkEYEnVBn8a8U8JlWbBUN86Kvo1C5ak45amJO8eLcXf6h7L0DA7TNzhC7+AIPYPD9A+OjLnULvdV7kBbzyB724Mmrt3tfezt6OPXzR209Qwe9fOqyhJUp4OlsixBImbEwyVmwWMiZvlPzGVz7vMz5UkaM2kW1qRpzKTD9TLmV6R08yYBFAoyGeXz4NSLgqWQO/S3B1OId+6F3gPBpbS9beF6WzAGY/uPgsdCsWTQgV5ZHyxVDUGAVDYEQZWuCZayzNj12Oz64jKzMExOPOBGskGTV2ffEB19Q3T2D9HZN0Rn3zCd/UN09Q+HS7DeMzjM8IgzknWGRrKMZJ2sO8MjQRzlQinXfOUe3JejpaufbJHaTSoeI52MUZFKBOeUDM6rIlwqw+2VZQnSyTjJmBGLjQZRzIxE3EjFY5Sn4pQl4qSTMcqTcdLJOJVlcarKklSlE1Qk45omZZZSKMjxMwtHYtfCwiNeHxD0a3Q0Q/tr0P6bYOluCZaeVmh5MVjPHn5lUV48FdQ2apePXWoWB4GRqoJUZbDMklHgxyIeM2rCZqOlR9/9uI1knQPdA+zr6GdfZz8tnf0c6h2id3CE/qERegeH6RvK0jc4TO/gCF39w7R0DoypDfUNTdyJPxlmUJVKUJUOgiYVj5GMx0jEjWQ8RioeIxYzsgWBN5z1fPBVliXyTWvBY9DsFgRaEERlyTDYknES8dG/ByuoRY1kncGRLAND2fBxhIHhLFl35lWkWFCZYn64ROViA4WCTI9keTCOom7lxPvkah597cGo7twy0Bk8du0LQuXQTtj1FAx0THAgGw2HWCIY02EGFg/WY4mgZjLvFKjNNWmF6+maoBz46CME70lWnJRhM148ZizMpFmYSfOG4zyGu5N1GM5myWaDx9wX9+Bwlr6hIGD6h4Iv2r6hIEy6B4bp7h+mK3zsHhiiZ3CE4ZEsQyNBAAyNBO8fznq+eSyViFEe1kjMjJ6BYXa397Flb1Cj6h4YPu6+ncmqTMWZX5UiE/bvBH1WQVNeVVmCVCKGYcQsGNRpFgRQMm75mlc6OfqYSsTCfYIaZ+4vK98MGA/ONxGL5dcz5aW/Ek6hILNHYc1jMvoOBQHRuQcGuoNO88HucL0nWPeR4PapnltGIDscBMzLP4auvZMvXywxrkkrA6nqsEnLwsCwghCKFQRRbPR5VcPYmk5V46xrFjsaMyNuEM/3C83sr+hs1ukOazJ9YU0mF0x9g0HAAAXBkVsx0skYqUSMskScskTQhAZGe+8gbT3BhQUHewZp6x7kYM9A0IQXhlL3wGiz3kixNrkp9rfvPIsbLjilpJ+hUJCTVy5Axl9VdSyG+qFjVzCTbfvOIEyAsV/yBiOD0N85tubS3xFcwZUPnIIahmcPX7JhIPWF78mJlwW1lcp6SJQFSzw1up5IBzWtRHnwmKwIHmPxoOmtaz907xt97DkQ7FOxILgarKIuXK8L/r1yfTTpmuBKsdz6SdrsBsEv80w6uFprpuRqT7nHrDvu5JulckGVC63B4WwQTQ7OaGhlnbDWlQ37i5yRbFCTWr+i9JeEKxQk2pLpozdrTbXhwTCIdhYsr0LvoaB209sWhFDuEuHh/mAZ6i1+vFQVVC0MruhqWhOEy1Bv2OHfBnt/FTz2tx+5XLmaUGFYpCqDwZHD/WMfs0PBPUHKa4N9cwFdPi8Ir0QqCLt4cjTkxiwF292D8x7qhcHe4Iq2wZ7g3yBZCWUFfUWpqiDwcv8+Q+G/zXBfUK70vODy6Oqm4L/tNMrVnsZP+1ZOHMpPnjEpCgWR6ZZIwYLTguVYuIdfhH3Bkh0KagFlVZN7/8hQWNsZ12fT3zG6ra997Hp3y2htJVUZ1DgSZUEtZaAr2Kdjd9CU198e1IRmi4q6cBzNoiDADqu9efBvkmtqHMw1O4aBVNUA1YuCkf2ZxUHQVDcGxx4eCMf2hGN8RgaDf6MxtbAwXBPp0c+koBYJ45oY46PPk+XB+2agWVGhIHKyMAubj45zdtx4MmhOqlwwteXKyf3iHx4IvzQHw1/0ufWh8At0qOC1cGxGqhJSFaM1gVRlUN6hvtEv61y/0VDfaPNa7sszN1VLX+4S6T3BFC+de4Kr3oZ6wy/ccMn1/cQTozWt1KnB55ZVB691twR9Tnufg20/DGoj0218k+FFt8DZ7y7tR5b06CISHWbBF+osuNXslMtdGdfdAtho81hh09hwf/Ea2PDAuIsPwlCCsRdA5NZHhgtqhD2jNcOh3mmZZkahICJyNJO5Mi6RCq5IK+kok9I7ua6DExGRklIoiIhInkJBRETyFAoiIpKnUBARkTyFgoiI5CkUREQkT6EgIiJ5Nv6m4rOdmbUCrx3n2+uAA1NYnJNFVM8bonvuOu9omcx5n+Lu9Uc70EkXCifCzDa7+7qZLsd0i+p5Q3TPXecdLVN53mo+EhGRPIWCiIjkRS0U7pzpAsyQqJ43RPfcdd7RMmXnHak+BRERObKo1RREROQIFAoiIpIXmVAws8vMbJuZ7TCzW2a6PKViZneZWYuZPV+wbb6Z/cjMtoePR7hTyMnJzJaa2SNmtsXMXjCzPw+3z+lzN7O0mT1lZr8Kz/uT4fYVZvZkeN7/bmapmS5rKZhZ3Mx+aWbfD5/P+fM2s51m9msze9bMNofbpuzvPBKhYGZx4A7gcuBM4L1mdubMlqpkvgpcNm7bLcCP3X0l8OPw+VwzDHzE3VcBFwB/Fv43nuvnPgC8xd3fAKwBLjOzC4B/AP4pPO9DwJ/MYBlL6c+BLQXPo3LeF7v7moKxCVP2dx6JUADWAzvc/RV3HwTuAa6e4TKVhLs/Bhwct/lq4Gvh+teAd05roaaBu+9192fC9S6CL4rFzPFz90B3+DQZLg68Bbgv3D7nzhvAzJYAVwL/Fj43InDeE5iyv/OohMJiYFfB8+ZwW1QsdPe9EHx5Ag0zXJ6SMrPlwFrgSSJw7mETyrNAC/Aj4GWg3d2Hw13m6t/7Z4GPAdnw+QKicd4OPGRmT5vZTeG2Kfs7T0xBAU8GVmSbrsWdg8ysCrgf+At37wx+PM5t7j4CrDGzecB3gFXFdpveUpWWmb0daHH3p83sotzmIrvOqfMOvcnd95hZA/AjM9s6lQePSk2hGVha8HwJsGeGyjIT9ptZE0D42DLD5SkJM0sSBMI33P3b4eZInDuAu7cDjxL0qcwzs9yPvrn49/4m4Coz20nQHPwWgprDXD9v3H1P+NhC8CNgPVP4dx6VUNgErAyvTEgB1wEbZrhM02kD8L5w/X3Af8xgWUoibE/+CrDF3T9T8NKcPnczqw9rCJhZOfBWgv6UR4B3h7vNufN291vdfYm7Lyf4//kn7v6HzPHzNrNKM6vOrQOXAs8zhX/nkRnRbGZXEPySiAN3ufvfzXCRSsLM7gYuIphKdz/wN8B3gXuBZcBvgPe4+/jO6JOamf028Djwa0bbmP+KoF9hzp67mZ1D0LEYJ/iRd6+7325mpxL8gp4P/BK4wd0HZq6kpRM2H93s7m+f6+cdnt93wqcJ4Jvu/ndmtoAp+juPTCiIiMjRRaX5SEREJkGhICIieQoFERHJUyiIiEieQkFERPIUCiLTyMwuys3oKTIbKRRERCRPoSBShJndEN6n4Fkz+1I46Vy3mf1PM3vGzH5sZvXhvmvM7Akze87MvpOby97MXmdmD4f3OnjGzE4LD19lZveZ2VYz+4ZFYYImOWkoFETGMbNVwLUEE4+tAUaAPwQqgWfc/VzgpwSjxQH+N/Bxdz+HYER1bvs3gDvCex38FrA33L4W+AuCe3ucSjCPj8isEJVZUkWOxe8B5wGbwh/x5QQTjGWBfw/3+TrwbTOrAea5+0/D7V8DvhXOT7PY3b8D4O79AOHxnnL35vD5s8By4GelPy2Ro1MoiBzOgK+5+61jNpp9Ytx+R5oj5khNQoVz8Yyg/w9lFlHzkcjhfgy8O5yvPnf/21MI/n/JzcB5PfAzd+8ADpnZ74TbbwR+6u6dQLOZvTM8RpmZVUzrWYgcB/1CERnH3V80s78muLtVDBgC/gzoAVab2dNAB0G/AwRTFX8x/NJ/BXh/uP1G4Etmdnt4jPdM42mIHBfNkioySWbW7e5VM10OkVJS85GIiOSppiAiInmqKYiISJ5CQURE8hQKIiKSp1AQEZE8hYKIiOT9/7qOVNDNe2SdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 1)\n"
     ]
    }
   ],
   "source": [
    "mse = np.mean(np.power(X_test - predictions, 2), axis=(1,2))\n",
    "print(mse.shape)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse.reshape((len(mse),)),\n",
    "                        'frameId': Id_test})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7350, 7365, 7380, 7425, 7440, 7500, 7515, 7530, 7545, 7560, 7575, 7590, 7605, 7620, 7635, 7650, 7740, 7815, 7845, 7860, 7875, 7890, 7905, 7920, 7935, 7950, 7980, 7995, 8010, 8025, 8040, 8055, 8130, 8160, 8175, 8190, 8205, 8235, 8250, 8340, 8355, 8370, 8385, 8400, 8415, 8430, 8445, 8460, 8475, 8490, 8505, 8520, 8535, 8550, 8565, 8580, 8595, 8610, 8625, 8640, 8655, 8670, 8760, 8775, 8790, 8805, 8820, 8865, 8880, 8895, 8910, 8940, 8955, 8970, 8985, 9000, 9015, 9030, 9045, 9060, 9105, 9120, 9135, 9150, 9165, 9180]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.015\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "pred_frameIds = []\n",
    "for y,frameId in zip(y_pred, Id_test):\n",
    "    if y==1:\n",
    "        pred_frameIds.append(frameId)\n",
    "pred_frameIds.sort()\n",
    "print(pred_frameIds)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
