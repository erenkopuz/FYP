{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52d3d61c31dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdarknet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDarknet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprep_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_to_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletterbox_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image, letterbox_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim\n",
    "\n",
    "def write(x, img):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = random.choice(colors)\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"video.avi\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"cfg/yolov3.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"yolov3.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
    "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
    "                        default = \"416\", type = str)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--video VIDEO] [--dataset DATASET]\n",
      "                             [--confidence CONFIDENCE]\n",
      "                             [--nms_thresh NMS_THRESH] [--cfg CFGFILE]\n",
      "                             [--weights WEIGHTSFILE] [--reso RESO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/Eren/Library/Jupyter/runtime/kernel-16db8a81-c358-43b1-a8db-14ac9259388c.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2918: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "confidence = float(0.7)\n",
    "nms_thesh = float(0.4)\n",
    "start = 0\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "bbox_attrs = 5 + num_classes\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(args.cfgfile)\n",
    "model.load_weights(args.weightsfile)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = args.reso\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model(get_test_input(inp_dim, CUDA), CUDA)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "videofile = args.video\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frames = 0\n",
    "start = time.time()    \n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "\n",
    "\n",
    "        img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "\n",
    "        im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
    "\n",
    "\n",
    "        if CUDA:\n",
    "            im_dim = im_dim.cuda()\n",
    "            img = img.cuda()\n",
    "\n",
    "        with torch.no_grad():   \n",
    "            output = model(Variable(img), CUDA)\n",
    "        output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "\n",
    "        if type(output) == int:\n",
    "            frames += 1\n",
    "            print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "            cv2.imshow(\"frame\", orig_im)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key & 0xFF == ord('q'):\n",
    "                break\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        im_dim = im_dim.repeat(output.size(0), 1)\n",
    "        scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
    "\n",
    "        output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "        output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "\n",
    "        output[:,1:5] /= scaling_factor\n",
    "\n",
    "        for i in range(output.shape[0]):\n",
    "            output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "            output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "\n",
    "        classes = load_classes('data/coco.names')\n",
    "        colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "        list(map(lambda x: write(x, orig_im), output))\n",
    "\n",
    "\n",
    "        cv2.imshow(\"frame\", orig_im)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "        frames += 1\n",
    "        print(\"FPS of the video is {:5.2f}\".format( frames / (time.time() - start)))\n",
    "\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "One image done\n",
      "307\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Object detection - YOLO - OpenCV\n",
    "# Author : Arun Ponnusamy   (July 16, 2018)\n",
    "# Website : http://www.arunponnusamy.com\n",
    "############################################\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "\n",
    "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "\n",
    "def get_yolo_output(image):\n",
    "\n",
    "    Width = image.shape[1]\n",
    "    Height = image.shape[0]\n",
    "    scale = 0.00392\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "\n",
    "    outs = net.forward(get_output_layers(net))\n",
    "\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    conf_threshold = 0.5\n",
    "    nms_threshold = 0.4\n",
    "\n",
    "\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * Width)\n",
    "                center_y = int(detection[1] * Height)\n",
    "                w = int(detection[2] * Width)\n",
    "                h = int(detection[3] * Height)\n",
    "                x = center_x - w / 2\n",
    "                y = center_y - h / 2\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([x, y, w, h])\n",
    "\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "    \n",
    "    boxes_nms = []\n",
    "    class_ids_nms = []\n",
    "    confidences_nms = []\n",
    "    \n",
    "    \n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        boxes_nms.append(boxes[i])\n",
    "        class_ids_nms.append(class_ids[i])\n",
    "        confidences_nms.append(confidences[i])\n",
    "    \n",
    "    return boxes_nms, class_ids_nms, confidences_nms\n",
    "\n",
    "\n",
    "def print_on_image(image, boxes_nms, class_ids_nms, confidences_nms):\n",
    "\n",
    "    cv2.polylines(image,swalk_pts,True,(0,255,255),thickness=3)\n",
    "    cv2.polylines(image,lane_pts,True,(255,0,255),thickness=3)\n",
    "    \n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "        x = box[0]\n",
    "        y = box[1]\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "        draw_prediction(image, class_ids_nms[i], confidences_nms[i], round(x), round(y), round(x+w), round(y+h))\n",
    "        \n",
    "\n",
    "        \n",
    "def create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor, grid_background):\n",
    "    \n",
    "    \n",
    "    grid_vehicles   = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_person     = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    grid_bicycle    = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "    \n",
    "    for i, box in enumerate(boxes_nms):\n",
    "\n",
    "        x = round(box[0])\n",
    "        y = round(box[1])\n",
    "        w = box[2]\n",
    "        h = box[3]\n",
    "\n",
    "        x_plus_w = round(x+w)\n",
    "        y_plus_h = round(y+h)\n",
    "\n",
    "\n",
    "        x = round(x/grid_scaling_factor)\n",
    "        y = round(y/grid_scaling_factor)\n",
    "        x_plus_w = round(x_plus_w/grid_scaling_factor)\n",
    "        y_plus_h = round(y_plus_h/grid_scaling_factor)\n",
    "        \n",
    "        if class_ids_nms[i] in [2, 3, 5, 6, 7]:  # Motorized vehicles [car, motorcycle, bus, train, truck]\n",
    "            \n",
    "            weight = vehicle_w\n",
    "            cv2.rectangle(grid_vehicles, (x,y), (x_plus_w,y_plus_h), vehicle_w, -1)\n",
    "\n",
    "\n",
    "        elif class_ids_nms[i]==0:  # Person\n",
    "            \n",
    "            weight = person_w\n",
    "            cv2.rectangle(grid_person, (x,y), (x_plus_w,y_plus_h), person_w, -1) \n",
    "            \n",
    "        elif class_ids_nms[i]==1:  # Bicycle\n",
    "            \n",
    "            weight = bicycle_w\n",
    "            cv2.rectangle(grid_bicycle, (x,y), (x_plus_w,y_plus_h), bicycle_w, -1)\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    grid_overflow = np.zeros((grid_y_size,grid_x_size), dtype = np.int16)    \n",
    "    grid_overflow = np.array([grid_background,grid_vehicles,grid_person,grid_bicycle])\n",
    "    grid_overflow = grid_overflow.sum(axis=0)\n",
    "    grid_overflow[grid_overflow>127] = 127\n",
    "    grid_overflow[grid_overflow<-128] = -128\n",
    "    grid = np.array(grid_overflow,dtype=np.int8)\n",
    "\n",
    "    return grid\n",
    "       \n",
    "    \n",
    "###############################################################################################    \n",
    "\n",
    "classes = None\n",
    "\n",
    "with open('yolov3.txt', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "GREYSCALE = np.random.uniform(0, 255, size=(len(classes)))\n",
    "\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "\n",
    "#image = cv2.imread('dog.jpg')\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "videofile = 'videos/4k_traffic_camera_video.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(videofile)\n",
    "\n",
    "assert cap.isOpened(), 'Cannot capture source'\n",
    "\n",
    "frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "success,image = cap.read()\n",
    "\n",
    "\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # Gets the frames per second\n",
    "num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) # Gets the total number of frames\n",
    "\n",
    "\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH) # Gets the frame width\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Gets the frame width\n",
    "\n",
    "seconds = 1\n",
    "grid_scaling_factor = 32\n",
    "grid_x_size = int(round(frame_width/grid_scaling_factor))\n",
    "grid_y_size = int(round(frame_height/grid_scaling_factor))\n",
    "\n",
    "\n",
    "multiplier = int(round(fps * seconds))\n",
    "num_scenes = int(round(num_frames/(fps * seconds))+1)\n",
    "\n",
    "\n",
    "grids = np.zeros((num_scenes,grid_y_size,grid_x_size), dtype = np.int8)\n",
    "\n",
    "\n",
    "\n",
    "########### Background Areas #############\n",
    "\n",
    "#lanes\n",
    "\n",
    "lane_pts = []\n",
    "lane_pts_1 = np.array([[540,0],[483,303],[314,555],[0,915],[0,1080],[625,1080],[708,719],[745,544],[763,362],[687,154],[791,310],[822,396],[835,582],[806,861],[779,1080],[1114,1080],[1060,664],[988,418],[924,317],[662,0]], np.int32)\n",
    "lane_pts_1 = lane_pts_1.reshape((-1,1,2))\n",
    "\n",
    "lane_pts_2 = np.array([[1920,600],[1832,660],[1760,773],[1692,907],[1660,1080],[1140,1080],[1245,893],[1418,691],[1590,569],[1782,480],[1920,421]], np.int32)\n",
    "lane_pts_2 = lane_pts_2.reshape((-1,1,2))\n",
    "\n",
    "lane_pts.append(lane_pts_1)\n",
    "lane_pts.append(lane_pts_2)\n",
    "\n",
    "\n",
    "lane_pts_grid = []\n",
    "\n",
    "for pts in lane_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    lane_pts_grid.append(grid_pts)\n",
    "\n",
    "\n",
    "\n",
    "#sidewalks\n",
    "\n",
    "swalk_pts = []\n",
    "swalk_pts_1 = np.array([[454,0],[540,0],[514,143],[448,149],[383,306],[483,303],[314,555],[0,915],[0,425],[179,356]], np.int32)\n",
    "swalk_pts_1 = swalk_pts_1.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_2 = np.array([[986,265],[1173,264],[1268,319],[1484,291],[1600,296],[1920,224],[1920,281],[1280,529],[1151,500]], np.int32)\n",
    "swalk_pts_2 = swalk_pts_2.reshape((-1,1,2))\n",
    "\n",
    "swalk_pts_3 = np.array([[1920,600],[1920,1080],[1660,1080],[1692,907],[1760,773],[1832,660]], np.int32)\n",
    "swalk_pts_3 = swalk_pts_3.reshape((-1,1,2))\n",
    "\n",
    "\n",
    "swalk_pts.append(swalk_pts_1)\n",
    "swalk_pts.append(swalk_pts_2)\n",
    "swalk_pts.append(swalk_pts_3)\n",
    "\n",
    "\n",
    "swalk_pts_grid = []\n",
    "\n",
    "for pts in swalk_pts:\n",
    "    grid_pts = np.divide(pts,grid_scaling_factor)\n",
    "    np.round_(grid_pts)\n",
    "    grid_pts = np.array(grid_pts,dtype=np.int32)\n",
    "    swalk_pts_grid.append(grid_pts)\n",
    "    \n",
    "\n",
    "\n",
    "#out of bounds (oob) areas\n",
    "\n",
    "\n",
    "#median spaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### Grid Weights #############\n",
    "\n",
    "\n",
    "lane_w = 32\n",
    "swalk_w = -32\n",
    "oob_w = -127\n",
    "med_sp_w = 0\n",
    "\n",
    "\n",
    "vehicle_w = 64\n",
    "person_w = -64\n",
    "bicycle_w = 32\n",
    "# obj_w = \n",
    "\n",
    "\n",
    "\n",
    "#################### Initiate Process ################\n",
    "\n",
    "#################### Create Background Grid ################\n",
    "\n",
    "grid_background = np.zeros((grid_y_size,grid_x_size), dtype = np.int8)\n",
    "\n",
    "# Mark background\n",
    "cv2.fillPoly(grid_background,lane_pts_grid,lane_w)\n",
    "cv2.fillPoly(grid_background,swalk_pts_grid,swalk_w)\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "i = 0\n",
    "\n",
    "while success:\n",
    "\n",
    "    if frameId % multiplier == 0:\n",
    "        \n",
    "\n",
    "        i= i+1\n",
    "#############################################################\n",
    "\n",
    "        #print (image.shape)\n",
    "        #print (type(image))\n",
    "        \n",
    "        boxes_nms, class_ids_nms, confidences_nms = get_yolo_output(image)\n",
    "        \n",
    "        grid = create_grid(boxes_nms, class_ids_nms, confidences_nms, grid_scaling_factor,grid_background)\n",
    "        grid_16_bit = np.array(grid,dtype=np.int16)\n",
    "        grid_16_bit = np.add(grid_16_bit,128)\n",
    "        grid_unsigned = np.array(grid_16_bit,dtype=np.uint8)\n",
    "        cv2.imwrite(\"output/grid%d.jpg\" % frameId, grid_unsigned)\n",
    "        \n",
    "        print_on_image(image, boxes_nms, class_ids_nms, confidences_nms)\n",
    "        \n",
    "        cv2.imwrite(\"output/frame%d.jpg\" % frameId, image)\n",
    "        \n",
    "        print ('One image done')\n",
    "        \n",
    "        \n",
    "    frameId = int(round(cap.get(1))) #current frame number, rounded b/c sometimes you get frame intervals which aren't integers...this adds a little imprecision but is likely good enough\n",
    "    success, image = cap.read()\n",
    "\n",
    "\n",
    "        #plt.figure(figsize = (15, 15))\n",
    "        #plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    \n",
    "#cv2.imwrite(\"object-detection.jpg\", image)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "print (i)\n",
    "\n",
    "cap.release()\n",
    "print (\"Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
